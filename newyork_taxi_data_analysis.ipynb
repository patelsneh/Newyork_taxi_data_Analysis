{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New York Taxi Data Analysis \n",
    "Group-9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generic modules\n",
    "import itertools\n",
    "import os\n",
    "import re\n",
    "import timeit\n",
    "import gc\n",
    "\n",
    "# specific module\n",
    "#import wget\n",
    "\n",
    "# common ds modules\n",
    "import pandas as pd\n",
    "#import plotly.express as px\n",
    "\n",
    "# spark modules for session managment\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# spark functions\n",
    "from pyspark.sql.functions import lit\n",
    "import pyspark.sql.functions as sparkle\n",
    "\n",
    "# spark types\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# spark ml\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.regression import GeneralizedLinearRegression\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n.config(\"spark.driver.maxResultSize\", \"8g\")     \\n    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")     .config(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"150000\")     .config(\"spark.sql.tungsten.enabled\", \"true\")     .config(\"spark.sql.shuffle.partitions\", \"360\")     .config(\"spark.rdd.compress\", \"true\") '"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# session starter named nyctaxi\n",
    "spark=SparkSession.builder \\\n",
    "    .appName('nyctaxi') \\\n",
    "    .master('local[*]') \\\n",
    "    .config('spark.driver.memory','10G') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "#     .config(\"spark.sql.default.parallelism\", \"360\") \\ \n",
    "'''\n",
    ".config(\"spark.driver.maxResultSize\", \"8g\") \\\n",
    "    \n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"150000\") \\\n",
    "    .config(\"spark.sql.tungsten.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"360\") \\\n",
    "    .config(\"spark.rdd.compress\", \"true\") \\\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Data from the website in to docker container\n",
    "https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: string, _c1: string, _c2: string, _c3: string, _c4: string, _c5: string, _c6: string, _c7: string, _c8: string, _c9: string, _c10: string, _c11: string, _c12: string, _c13: string, _c14: string, _c15: string, _c16: string, _c17: string]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#spark.read.csv(\"Dataset/yellow-2019-01.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting CSV to Initial Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reads directory, filters for csv's and feeds into loop to convert to parquet\n",
    "files=[re.search(r\"(.*)(\\.csv)$\", file).group(1) for file in os.listdir(\"./Dataset/\") if file.endswith(\".csv\")]\n",
    "for file in files:\n",
    "    inpath = f\"./Dataset/{file}.csv\"\n",
    "    readdf = spark.read.csv(inpath, header = \"true\")\n",
    "    outpath = f\"./Dataset/prq/{file}.parquet\"\n",
    "    readdf.write.parquet(outpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "colours=['yellow']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Dirty Data with color of taxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combines month data into single file per colour\n",
    "for colour in colours:\n",
    "    # uses a sample of the first dataset to create and empty df with correct format to join to\n",
    "    initpath = f\"./Dataset/prq/{colour}-2019-01.parquet\"\n",
    "    outdf = spark.read.parquet(initpath)\n",
    "    outdf = outdf.limit(0)\n",
    "    # get files for loop\n",
    "    files = [re.search(r\"(.*)(\\.parquet)$\", file).group(1) for file in os.listdir(\"./Dataset/prq\") if file.endswith(\".parquet\") and file.startswith(colour)]\n",
    "    for file in files:\n",
    "        inpath = f\"./Dataset/prq/{file}.parquet\"\n",
    "        readdf = spark.read.parquet(inpath)\n",
    "        # !! unionByName !! ensures columns match union method can result in incorrect mapping\n",
    "        outdf = outdf.unionByName(readdf)\n",
    "    outpath = f\"./data/{colour}-all.parquet\"\n",
    "    outdf.write.parquet(outpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['VendorID',\n",
       " 'tpep_pickup_datetime',\n",
       " 'tpep_dropoff_datetime',\n",
       " 'passenger_count',\n",
       " 'trip_distance',\n",
       " 'RatecodeID',\n",
       " 'store_and_fwd_flag',\n",
       " 'PULocationID',\n",
       " 'DOLocationID',\n",
       " 'payment_type',\n",
       " 'fare_amount',\n",
       " 'extra',\n",
       " 'mta_tax',\n",
       " 'tip_amount',\n",
       " 'tolls_amount',\n",
       " 'improvement_surcharge',\n",
       " 'total_amount',\n",
       " 'congestion_surcharge']"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read and check columns\n",
    "yellowdf = spark.read.parquet(\"./data/yellow-all.parquet\")\n",
    "yellowdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass data to next stage\n",
    "data = yellowdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uncleaned Full Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7667792"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check record numbers match\n",
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform dataframe \n",
    "# add new missing columns with releveant value\n",
    "yellowdf = yellowdf.withColumn('trip_type', lit(\"1\"))\n",
    "yellowdf = yellowdf.withColumn('ehail_fee', lit(\"0\"))\n",
    "# create colour variable to track dataset\n",
    "yellowdf = yellowdf.withColumn('colour', lit(\"yellow\"))\n",
    "yellowdf = yellowdf.withColumnRenamed(\"tpep_pickup_datetime\", \"pickup_datetime\")\n",
    "yellowdf = yellowdf.withColumnRenamed(\"tpep_dropoff_datetime\", \"dropoff_datetime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass data to next stage\n",
    "data = yellowdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: string (nullable = true)\n",
      " |-- pickup_datetime: string (nullable = true)\n",
      " |-- dropoff_datetime: string (nullable = true)\n",
      " |-- passenger_count: string (nullable = true)\n",
      " |-- trip_distance: string (nullable = true)\n",
      " |-- RatecodeID: string (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: string (nullable = true)\n",
      " |-- DOLocationID: string (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- fare_amount: string (nullable = true)\n",
      " |-- extra: string (nullable = true)\n",
      " |-- mta_tax: string (nullable = true)\n",
      " |-- tip_amount: string (nullable = true)\n",
      " |-- tolls_amount: string (nullable = true)\n",
      " |-- improvement_surcharge: string (nullable = true)\n",
      " |-- total_amount: string (nullable = true)\n",
      " |-- congestion_surcharge: string (nullable = true)\n",
      " |-- trip_type: string (nullable = false)\n",
      " |-- ehail_fee: string (nullable = false)\n",
      " |-- colour: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# intial schema not imputed as no cleaning done\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create view for spark.sql queries\n",
    "data.createOrReplaceTempView(\"data_init_view\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping by Colour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+\n",
      "|colour|count(colour)|\n",
      "+------+-------------+\n",
      "|yellow|      7667792|\n",
      "+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SQL query, group by relevent variable and create count to check splits\n",
    "spark.sql(\"\"\"\n",
    "            SELECT colour, count(colour)\n",
    "            FROM data_init_view\n",
    "            GROUP by colour\n",
    "        \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VendorID\n",
    "Should be 1 or 2\n",
    "   - 1-Creative Mobile Technologies\n",
    "   - 2-Verifone INC.\n",
    "    \n",
    "- VendorId=4 contains 76823 records?\n",
    "- Ratecodes include 99 for VendorId=4 which is invalid it should be in range of 1-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+\n",
      "|VendorID|count(VendorID)|\n",
      "+--------+---------------+\n",
      "|       1|        2938778|\n",
      "|       4|          76823|\n",
      "|       2|        4652191|\n",
      "+--------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "            SELECT VendorID, count(VendorID)\n",
    "            FROM data_init_view\n",
    "            GROUP by VendorID\n",
    "        \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+---------+---------+------+\n",
      "|VendorID|    pickup_datetime|   dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|trip_type|ehail_fee|colour|\n",
      "+--------+-------------------+-------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+---------+---------+------+\n",
      "|       4|2019-01-25 17:00:59|2019-01-25 17:04:53|              1|          .58|         1|                 N|         237|         262|           2|        4.5|    1|    0.5|         0|           0|                  0.3|         6.3|                null|        1|        0|yellow|\n",
      "|       4|2019-01-25 17:10:44|2019-01-25 17:15:40|              1|          .76|         1|                 N|         262|         236|           2|          5|    1|    0.5|         0|           0|                  0.3|         6.8|                null|        1|        0|yellow|\n",
      "|       4|2019-01-25 17:19:17|2019-01-25 17:33:24|              1|         2.87|         1|                 N|         237|          41|           1|         12|    1|    0.5|         2|           0|                  0.3|        15.8|                null|        1|        0|yellow|\n",
      "|       4|2019-01-25 17:00:30|2019-01-25 17:06:48|              1|          .65|         1|                 N|         163|         230|           2|        5.5|    1|    0.5|         0|           0|                  0.3|         7.3|                null|        1|        0|yellow|\n",
      "|       4|2019-01-25 17:09:49|2019-01-25 17:19:38|              1|         1.97|         1|                 N|         230|          68|           1|        8.5|    1|    0.5|       1.5|           0|                  0.3|        11.8|                null|        1|        0|yellow|\n",
      "|       4|2019-01-25 17:29:57|2019-01-25 17:35:34|              1|          .84|         1|                 N|          68|         246|           1|        5.5|    1|    0.5|         1|           0|                  0.3|         8.3|                null|        1|        0|yellow|\n",
      "|       4|2019-01-25 17:39:09|2019-01-25 17:46:31|              1|         1.07|         1|                 N|         246|         186|           1|          7|    1|    0.5|         1|           0|                  0.3|         9.8|                null|        1|        0|yellow|\n",
      "|       4|2019-01-25 17:56:18|2019-01-25 18:08:10|              1|         1.44|         1|                 N|         186|         163|           2|          9|    1|    0.5|         0|           0|                  0.3|        10.8|                null|        1|        0|yellow|\n",
      "|       4|2019-01-25 17:03:13|2019-01-25 17:12:16|              1|         1.40|         1|                 N|         113|         137|           1|          8|    1|    0.5|      1.96|           0|                  0.3|       11.76|                null|        1|        0|yellow|\n",
      "|       4|2019-01-25 17:19:07|2019-01-25 17:33:57|              1|         1.24|         1|                 N|         170|         161|           2|         10|    1|    0.5|         0|           0|                  0.3|        11.8|                null|        1|        0|yellow|\n",
      "|       4|2019-01-25 17:34:29|2019-01-25 17:43:32|              1|         1.49|         1|                 N|         161|         143|           1|          8|    1|    0.5|      1.96|           0|                  0.3|       11.76|                null|        1|        0|yellow|\n",
      "|       4|2019-01-25 17:59:49|2019-01-25 18:12:17|              2|         1.27|         1|                 N|         230|         170|           2|          9|    1|    0.5|         0|           0|                  0.3|        10.8|                null|        1|        0|yellow|\n",
      "|       4|2019-01-25 17:27:23|2019-01-25 17:33:47|              1|          .37|         1|                 N|         237|         162|           2|        5.5|    1|    0.5|         0|           0|                  0.3|         7.3|                null|        1|        0|yellow|\n",
      "|       4|2019-01-25 17:36:03|2019-01-25 17:46:36|              1|         1.26|         1|                 N|         162|         163|           1|          8|    1|    0.5|      2.94|           0|                  0.3|       12.74|                null|        1|        0|yellow|\n",
      "|       4|2019-01-25 17:49:49|2019-01-25 17:59:25|              1|         1.06|         1|                 N|         163|         141|           1|        7.5|    1|    0.5|      1.86|           0|                  0.3|       11.16|                null|        1|        0|yellow|\n",
      "|       4|2019-01-25 17:16:15|2019-01-25 17:33:08|              1|         1.29|         1|                 N|         100|         249|           1|       11.5|    1|    0.5|      2.66|           0|                  0.3|       15.96|                null|        1|        0|yellow|\n",
      "|       4|2019-01-25 17:44:03|2019-01-25 17:49:12|              1|          .28|         1|                 N|          90|          90|           2|        4.5|    1|    0.5|         0|           0|                  0.3|         6.3|                null|        1|        0|yellow|\n",
      "|       4|2019-01-25 17:59:34|2019-01-25 18:10:18|              1|         1.19|         1|                 N|         186|         170|           1|        8.5|    1|    0.5|      2.06|           0|                  0.3|       12.36|                null|        1|        0|yellow|\n",
      "|       4|2019-01-25 17:07:58|2019-01-25 17:57:40|              1|         9.32|         1|                 N|         138|         230|           2|         35|    1|    0.5|         0|        5.76|                  0.3|       42.56|                null|        1|        0|yellow|\n",
      "|       4|2019-01-25 17:41:20|2019-01-25 18:02:32|              1|         2.31|         1|                 N|         162|         246|           1|       14.5|    1|    0.5|      3.26|           0|                  0.3|       19.56|                null|        1|        0|yellow|\n",
      "+--------+-------------------+-------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+---------+---------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "            SELECT *\n",
    "            FROM data_init_view\n",
    "            WHERE VendorID == 4\n",
    "        \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+\n",
      "|colour|count(colour)|\n",
      "+------+-------------+\n",
      "|yellow|        76823|\n",
      "+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "            SELECT colour, count(colour)\n",
    "            FROM data_init_view\n",
    "            WHERE VendorID == 4\n",
    "            GROUP by colour\n",
    "        \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+\n",
      "|RatecodeID|count(RatecodeID)|\n",
      "+----------+-----------------+\n",
      "|         3|               78|\n",
      "|        99|                5|\n",
      "|         5|              225|\n",
      "|         1|            75240|\n",
      "|         4|               46|\n",
      "|         2|             1229|\n",
      "+----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "            SELECT RatecodeID, count(RatecodeID)\n",
    "            FROM data_init_view\n",
    "            WHERE VendorID == 4\n",
    "            GROUP by RatecodeID\n",
    "        \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------------------+\n",
      "|passenger_count|count(passenger_count)|\n",
      "+---------------+----------------------+\n",
      "|              3|                   266|\n",
      "|              5|                     9|\n",
      "|              1|                 75449|\n",
      "|              4|                   107|\n",
      "|              2|                   992|\n",
      "+---------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "            SELECT passenger_count, count(passenger_count)\n",
    "            FROM data_init_view\n",
    "            WHERE VendorID == 4\n",
    "            GROUP by passenger_count\n",
    "        \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|MonthGroup|count|\n",
      "+----------+-----+\n",
      "|       jan|76823|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "            CREATE OR REPLACE TEMPORARY VIEW vid4months\n",
    "            AS\n",
    "            SELECT pickup_datetime,\n",
    "                CASE\n",
    "                    WHEN pickup_datetime LIKE '%2019-01%' THEN 'jan'\n",
    "                    ELSE \"unknown\"\n",
    "                END AS MonthGroup\n",
    "            FROM data_init_view\n",
    "            WHERE VendorID == 4\n",
    "        \"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "            SELECT MonthGroup, count(MonthGroup) as count\n",
    "            FROM vid4months\n",
    "            GROUP BY MonthGroup\n",
    "            ORDER BY count\n",
    "        \"\"\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passenger Count\n",
    "- 117381 records of passenger count 0\n",
    "- For 7-9 passenger count\n",
    "    - ~57 records( What are they Maxi Type?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------------------+\n",
      "|passenger_count|count(passenger_count)|\n",
      "+---------------+----------------------+\n",
      "|              7|                    19|\n",
      "|              3|                314721|\n",
      "|              8|                    29|\n",
      "|              0|                117381|\n",
      "|              5|                323842|\n",
      "|              6|                200811|\n",
      "|              9|                     9|\n",
      "|              1|               5456121|\n",
      "|              4|                140753|\n",
      "|              2|               1114106|\n",
      "+---------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "            SELECT passenger_count, count(passenger_count)\n",
    "            FROM data_init_view\n",
    "            GROUP by passenger_count\n",
    "        \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RatecodeID should be in range(1-6)\n",
    "- 252 with RatecodeID=99\n",
    "- 145 with distance 0\n",
    "- 84 PULocation 264"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+\n",
      "|RatecodeID|count(RatecodeID)|\n",
      "+----------+-----------------+\n",
      "|         3|            11801|\n",
      "|        99|              252|\n",
      "|         5|            54569|\n",
      "|         6|               46|\n",
      "|         1|          7430139|\n",
      "|         4|             4895|\n",
      "|         2|           166090|\n",
      "+----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "            SELECT RatecodeID, count(RatecodeID)\n",
    "            FROM data_init_view\n",
    "            GROUP by RatecodeID\n",
    "        \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|colour|count|\n",
      "+------+-----+\n",
      "|yellow|  252|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "            SELECT colour, count(colour) as count\n",
    "            FROM data_init_view\n",
    "            WHERE NOT RatecodeID BETWEEN 1 AND 6\n",
    "            GROUP BY colour\n",
    "        \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|trip_distance|count|\n",
      "+-------------+-----+\n",
      "|          .00|  145|\n",
      "|          .84|    4|\n",
      "|         1.27|    4|\n",
      "|         1.35|    3|\n",
      "|          .66|    2|\n",
      "|          .72|    2|\n",
      "|          .74|    2|\n",
      "|         1.50|    2|\n",
      "|          .94|    2|\n",
      "|          .73|    2|\n",
      "|         3.84|    2|\n",
      "|         2.57|    2|\n",
      "|         2.19|    2|\n",
      "|         1.18|    2|\n",
      "|         1.52|    2|\n",
      "|         2.78|    1|\n",
      "|         1.30|    1|\n",
      "|        23.89|    1|\n",
      "|         1.68|    1|\n",
      "|         1.74|    1|\n",
      "+-------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "            SELECT trip_distance, count(trip_distance) AS count\n",
    "            FROM data_init_view\n",
    "            WHERE NOT RatecodeID BETWEEN 1 AND 6\n",
    "            GROUP by trip_distance\n",
    "            ORDER BY count DESC\n",
    "        \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|PULocationID|count|\n",
      "+------------+-----+\n",
      "|         264|   84|\n",
      "|         265|   16|\n",
      "|         142|    8|\n",
      "|         239|    8|\n",
      "|         170|    7|\n",
      "|          43|    7|\n",
      "|          79|    7|\n",
      "|         231|    6|\n",
      "|         162|    6|\n",
      "|         230|    6|\n",
      "|         138|    5|\n",
      "|         193|    5|\n",
      "|         132|    5|\n",
      "|         107|    5|\n",
      "|         161|    5|\n",
      "|         234|    4|\n",
      "|         141|    4|\n",
      "|         237|    4|\n",
      "|         145|    4|\n",
      "|         238|    3|\n",
      "+------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "            SELECT PULocationID, count(PULocationID) AS count\n",
    "            FROM data_init_view\n",
    "            WHERE NOT RatecodeID BETWEEN 1 AND 6\n",
    "            GROUP by PULocationID\n",
    "            ORDER BY count DESC\n",
    "        \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Payment Type should be in range(1-6)\n",
    "- All valid within range\n",
    "- No 6 \n",
    "- Count descends as the payment type increases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+\n",
      "|payment_type|  count|\n",
      "+------------+-------+\n",
      "|           1|5486027|\n",
      "|           2|2137415|\n",
      "|           3|  33186|\n",
      "|           4|  11164|\n",
      "+------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "            SELECT payment_type, count(payment_type) AS count\n",
    "            FROM data_init_view\n",
    "            GROUP by payment_type\n",
    "            ORDER BY count DESC\n",
    "        \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra, it should be 0.5 or 1\n",
    "- If all the values is valid then we can change to two bools[]\n",
    "- Out of range value including negatives, Overnight Charges?\n",
    "- 37 unique values \n",
    "- 7 negative values\n",
    "- Valid values\n",
    "    - 1316580 records contains 1\n",
    "    - 2116494 records contains 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+\n",
      "|extra|  count|\n",
      "+-----+-------+\n",
      "|    0|4199855|\n",
      "|  0.5|2116494|\n",
      "|    1|1316580|\n",
      "|  4.5|  31241|\n",
      "| -0.5|   2201|\n",
      "|   -1|    863|\n",
      "|  0.8|    229|\n",
      "| -4.5|     79|\n",
      "|  1.3|     74|\n",
      "| 17.5|     63|\n",
      "|  1.8|     34|\n",
      "|  2.5|     21|\n",
      "|  0.3|     10|\n",
      "|   18|      9|\n",
      "|    3|      7|\n",
      "| 18.5|      6|\n",
      "|  5.3|      4|\n",
      "|  0.2|      3|\n",
      "| 0.25|      1|\n",
      "| 10.9|      1|\n",
      "+-----+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "            SELECT extra, count(extra) AS count\n",
    "            FROM data_init_view\n",
    "            GROUP by extra\n",
    "            ORDER BY count DESC\n",
    "        \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "            SELECT extra, count(extra) AS count\n",
    "            FROM data_init_view\n",
    "            GROUP by extra\n",
    "            ORDER BY count DESC\n",
    "        \"\"\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "            SELECT extra, count(extra) AS count\n",
    "            FROM data_init_view\n",
    "            WHERE extra < 0\n",
    "            GROUP by extra\n",
    "            ORDER BY count DESC\n",
    "        \"\"\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mta_Tax should be 0.5\n",
    "- If all values are valid change to bool[]\n",
    "- 7625883 valid values\n",
    "- 34984: 0 values\n",
    "- 6819: -0.5 value(Refund?)\n",
    "    - Check other out of range and negative values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|mta_tax|  count|\n",
      "+-------+-------+\n",
      "|    0.5|7625883|\n",
      "|      0|  34984|\n",
      "|   -0.5|   6819|\n",
      "|   0.25|     97|\n",
      "|   0.35|      2|\n",
      "|  32.53|      1|\n",
      "|  37.51|      1|\n",
      "|    0.9|      1|\n",
      "|   2.42|      1|\n",
      "|   60.8|      1|\n",
      "|      1|      1|\n",
      "|   18.3|      1|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "            SELECT mta_tax, count(mta_tax) AS count\n",
    "            FROM data_init_view\n",
    "            GROUP by mta_tax\n",
    "            ORDER BY count DESC\n",
    "        \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out of range value\n",
    "- 6925: Out of range value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|colour|count|\n",
      "+------+-----+\n",
      "|yellow| 6925|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "            SELECT colour, count(colour) AS count\n",
    "            FROM data_init_view\n",
    "            WHERE mta_tax != \"0\"\n",
    "            AND mta_tax != \"0.5\"\n",
    "            GROUP by colour\n",
    "            ORDER BY count DESC\n",
    "        \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improvement_Surcharge should be 0.3\n",
    "- If all values are valid change to bool[]\n",
    "- 7658005 valid 0.3 values\n",
    "- 7129: -0.3 value(Refund?)\n",
    "- 2657: 0 value\n",
    "- 1: 0.6 value\n",
    "    - All 0 trip_diatance\n",
    "    - All PU/Do Id= 265"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-------+\n",
      "|improvement_surcharge|  count|\n",
      "+---------------------+-------+\n",
      "|                  0.3|7658005|\n",
      "|                 -0.3|   7129|\n",
      "|                    0|   2657|\n",
      "|                  0.6|      1|\n",
      "+---------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "            SELECT improvement_surcharge, count(improvement_surcharge) AS count\n",
    "            FROM data_init_view\n",
    "            GROUP by improvement_surcharge\n",
    "            ORDER BY count DESC\n",
    "        \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+----------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+---------+---------+------+\n",
      "|VendorID|pickup_datetime|dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|trip_type|ehail_fee|colour|\n",
      "+--------+---------------+----------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+---------+---------+------+\n",
      "+--------+---------------+----------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+---------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "            SELECT *\n",
    "            FROM data_init_view\n",
    "            WHERE improvement_surcharge = \"1\"\n",
    "        \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trip Type should be 1 for Yellow Taxi\n",
    "- All are valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n",
      "|trip_type|  count|\n",
      "+---------+-------+\n",
      "|        1|7667792|\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "            SELECT trip_type, count(trip_type) AS count\n",
    "            FROM data_init_view\n",
    "            GROUP by trip_type\n",
    "            ORDER BY count DESC\n",
    "        \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Location Value\n",
    "(Should be an integer from 1-265)- from Taxizone lookup Table:-\n",
    "    https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\n",
    "- PULocationID\n",
    "- DULocationID\n",
    "- Both\n",
    "    - No non integer values \n",
    "    - No null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+----------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+---------+---------+------+\n",
      "|VendorID|pickup_datetime|dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|trip_type|ehail_fee|colour|\n",
      "+--------+---------------+----------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+---------+---------+------+\n",
      "+--------+---------------+----------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+---------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# look for non integer values\n",
    "spark.sql(\"\"\"\n",
    "            SELECT *\n",
    "            FROM data_init_view\n",
    "            WHERE PULocationID BETWEEN 1 AND 265\n",
    "            and mod(PULocationID, 1) != 0\n",
    "        \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+\n",
      "|PULocationID| count|\n",
      "+------------+------+\n",
      "|         237|332473|\n",
      "|         236|323008|\n",
      "|         161|312392|\n",
      "|         162|277166|\n",
      "|         230|263646|\n",
      "|         186|260712|\n",
      "|          48|240903|\n",
      "|         170|238978|\n",
      "|         234|237648|\n",
      "|         142|235144|\n",
      "|         239|207883|\n",
      "|         163|199682|\n",
      "|         132|196612|\n",
      "|          79|193955|\n",
      "|         141|192380|\n",
      "|         138|184334|\n",
      "|         107|176786|\n",
      "|         164|172647|\n",
      "|          68|171971|\n",
      "|         238|162192|\n",
      "+------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# count of location values, most first\n",
    "spark.sql(\"\"\"\n",
    "            SELECT PULocationID, count(PULocationID) AS count\n",
    "            FROM data_init_view\n",
    "            GROUP by PULocationID\n",
    "            ORDER BY count DESC\n",
    "        \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+----------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+---------+---------+------+\n",
      "|VendorID|pickup_datetime|dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|trip_type|ehail_fee|colour|\n",
      "+--------+---------------+----------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+---------+---------+------+\n",
      "+--------+---------------+----------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+---------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "            SELECT *\n",
    "            FROM data_init_view\n",
    "            WHERE NOT DOLocationID BETWEEN 1 AND 265\n",
    "        \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+\n",
      "|DOLocationID| count|\n",
      "+------------+------+\n",
      "|         236|334323|\n",
      "|         237|296185|\n",
      "|         161|293782|\n",
      "|         170|242037|\n",
      "|         162|232451|\n",
      "|         230|225336|\n",
      "|         142|214164|\n",
      "|          48|208624|\n",
      "|         234|204386|\n",
      "|         239|204350|\n",
      "|         141|202184|\n",
      "|         186|189486|\n",
      "|         163|175754|\n",
      "|         238|175310|\n",
      "|          79|168608|\n",
      "|          68|167144|\n",
      "|         107|162697|\n",
      "|         263|158297|\n",
      "|         164|154200|\n",
      "|         140|152042|\n",
      "+------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "            SELECT DOLocationID, count(DOLocationID) AS count\n",
    "            FROM data_init_view\n",
    "            GROUP by DOLocationID\n",
    "            ORDER BY count DESC\n",
    "        \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+\n",
      "|DOLocationID| count|\n",
      "+------------+------+\n",
      "|         264|149094|\n",
      "|         265| 16817|\n",
      "+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "            SELECT DOLocationID, count(DOLocationID) AS count\n",
    "            FROM data_init_view\n",
    "            WHERE DOLocationID >= 264\n",
    "            GROUP by DOLocationID\n",
    "            ORDER BY count DESC\n",
    "        \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check dates are within range:-\n",
    "(Should be 2019-01-01 00:00:00 to 2019-01-31 23:59:59)\n",
    "- Pickup_datetime\n",
    "    - min 2001-02-02 14:55:07\n",
    "        - 441 low records\n",
    "    - Max 2088-01-24 00:25:39\n",
    "        - 96 high records\n",
    "- Dropoff_datetime\n",
    "    - min 2001-02-02 15:07:27\n",
    "        - 316 low records\n",
    "    - Max 2088-01-24 07:28:25\n",
    "        - 3011 hign records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---------------------+---------------------+\n",
      "|min(pickup_datetime)|max(pickup_datetime)|min(dropoff_datetime)|max(dropoff_datetime)|\n",
      "+--------------------+--------------------+---------------------+---------------------+\n",
      "| 2001-02-02 14:55:07| 2088-01-24 00:25:39|  2001-02-02 15:07:27|  2088-01-24 07:28:25|\n",
      "+--------------------+--------------------+---------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "            SELECT MIN(pickup_datetime), MAX(pickup_datetime), MIN(dropoff_datetime),  MAX(dropoff_datetime)\n",
    "            FROM data_init_view\n",
    "        \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+\n",
      "|tripdays|  count|\n",
      "+--------+-------+\n",
      "|       0|7597276|\n",
      "|       1|  70508|\n",
      "|     -58|      1|\n",
      "|      -2|      1|\n",
      "|     -19|      1|\n",
      "|      30|      1|\n",
      "|      22|      1|\n",
      "|      24|      1|\n",
      "|       5|      1|\n",
      "|       2|      1|\n",
      "+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# calculate tripdays using datediff (simpler)\n",
    "spark.sql(\"\"\"\n",
    "            WITH tripdaysTable AS (\n",
    "            SELECT *, datediff(dropoff_datetime, pickup_datetime) as tripdays\n",
    "            FROM data_init_view\n",
    "            )\n",
    "            SELECT tripdays, count(tripdays) AS count\n",
    "            FROM tripdaysTable\n",
    "            GROUP by tripdays\n",
    "            ORDER BY count DESC\n",
    "        \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|sum(count)|\n",
      "+----------+\n",
      "|         8|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "            WITH countsTable AS (\n",
    "                WITH tripdaysTable AS (\n",
    "                    SELECT *, datediff(dropoff_datetime, pickup_datetime) as tripdays\n",
    "                    FROM data_init_view\n",
    "                    )\n",
    "                SELECT tripdays, count(tripdays) AS count\n",
    "                FROM tripdaysTable\n",
    "                GROUP by tripdays\n",
    "                ORDER BY count DESC\n",
    "            )\n",
    "            SELECT sum(count)\n",
    "            FROM countsTable\n",
    "            WHERE tripdays != \"0\"\n",
    "            AND tripdays != \"1\"\n",
    "        \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+---------+---------+------+\n",
      "|VendorID|    pickup_datetime|   dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|trip_type|ehail_fee|colour|\n",
      "+--------+-------------------+-------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+---------+---------+------+\n",
      "|       2|2018-12-31 23:59:58|2019-01-01 00:03:52|              1|          .66|         1|                 N|         162|         170|           2|        4.5|  0.5|    0.5|         0|           0|                  0.3|         5.8|                null|        1|        0|yellow|\n",
      "|       2|2018-12-31 23:59:57|2019-01-01 00:22:00|              1|         8.37|         1|                 N|         239|         235|           2|         26|  0.5|    0.5|         0|           0|                  0.3|        27.3|                null|        1|        0|yellow|\n",
      "|       2|2018-12-31 23:59:57|2019-01-01 00:00:00|              4|          .00|         5|                 N|         264|         265|           1|        121|    0|    0.5|     36.54|           0|                  0.3|      158.34|                null|        1|        0|yellow|\n",
      "|       2|2018-12-31 23:59:52|2019-01-01 00:00:29|              1|          .06|         1|                 N|         164|         164|           1|        2.5|  0.5|    0.5|         0|           0|                  0.3|         3.8|                null|        1|        0|yellow|\n",
      "|       2|2018-12-31 23:59:52|2019-01-01 00:13:08|              3|         3.32|         1|                 N|           7|         129|           2|       12.5|  0.5|    0.5|         0|           0|                  0.3|        13.8|                null|        1|        0|yellow|\n",
      "|       2|2018-12-31 23:59:51|2019-01-01 00:03:48|              2|          .35|         1|                 N|         249|         249|           1|          4|  0.5|    0.5|      1.06|           0|                  0.3|        6.36|                null|        1|        0|yellow|\n",
      "|       2|2018-12-31 23:59:47|2019-01-01 00:13:02|              1|         4.46|         1|                 N|          65|          79|           2|         15|  0.5|    0.5|         0|           0|                  0.3|        16.3|                null|        1|        0|yellow|\n",
      "|       2|2018-12-31 23:59:46|2019-01-01 00:08:54|              2|         1.06|         1|                 N|         161|         142|           2|        7.5|  0.5|    0.5|         0|           0|                  0.3|         8.8|                null|        1|        0|yellow|\n",
      "|       2|2018-12-31 23:59:38|2019-01-01 00:14:29|              1|          .61|         1|                 N|          68|          68|           1|         10|  0.5|    0.5|      2.26|           0|                  0.3|       13.56|                null|        1|        0|yellow|\n",
      "|       2|2018-12-31 23:59:38|2019-01-01 00:13:31|              2|         6.90|         1|                 N|         132|         197|           2|       20.5|  0.5|    0.5|         0|           0|                  0.3|        21.8|                null|        1|        0|yellow|\n",
      "|       2|2018-12-31 23:59:38|2019-01-01 00:50:35|              1|         3.97|         1|                 N|         142|          79|           2|         29|  0.5|    0.5|         0|           0|                  0.3|        30.3|                null|        1|        0|yellow|\n",
      "|       2|2018-12-31 23:59:36|2019-01-01 00:16:24|              1|         3.77|         1|                 N|          48|         263|           1|         15|  0.5|    0.5|         2|           0|                  0.3|        18.3|                null|        1|        0|yellow|\n",
      "|       2|2018-12-31 23:59:36|2019-01-01 00:23:51|              1|        12.95|         1|                 N|         148|         136|           2|       36.5|  0.5|    0.5|         0|           0|                  0.3|        37.8|                null|        1|        0|yellow|\n",
      "|       2|2018-12-31 23:59:31|2019-01-01 00:13:13|              1|         4.54|         1|                 N|          48|          74|           2|       14.5|  0.5|    0.5|         0|           0|                  0.3|        15.8|                null|        1|        0|yellow|\n",
      "|       2|2018-12-31 23:59:30|2019-01-01 00:03:09|              1|         1.06|         1|                 N|         229|         140|           1|          5|  0.5|    0.5|      1.89|           0|                  0.3|        8.19|                null|        1|        0|yellow|\n",
      "|       2|2018-12-31 23:59:30|2019-01-01 00:04:09|              1|         1.27|         1|                 N|          68|         158|           1|          6|  0.5|    0.5|      1.46|           0|                  0.3|        8.76|                null|        1|        0|yellow|\n",
      "|       2|2018-12-31 23:59:26|2019-01-01 00:13:26|              2|         2.24|         1|                 N|         186|         224|           1|         11|  0.5|    0.5|      3.08|           0|                  0.3|       15.38|                null|        1|        0|yellow|\n",
      "|       2|2018-12-31 23:59:24|2019-01-01 00:31:15|              2|        21.09|         2|                 N|         132|         243|           2|         52|    0|    0.5|         0|        5.76|                  0.3|       58.56|                null|        1|        0|yellow|\n",
      "|       2|2018-12-31 23:59:23|2019-01-01 00:07:25|              1|         3.04|         1|                 N|          79|         140|           1|         10|  0.5|    0.5|      2.82|           0|                  0.3|       14.12|                null|        1|        0|yellow|\n",
      "|       2|2018-12-31 23:59:19|2019-01-01 00:04:32|              1|          .90|         1|                 N|         148|          79|           2|        5.5|  0.5|    0.5|         0|           0|                  0.3|         6.8|                null|        1|        0|yellow|\n",
      "+--------+-------------------+-------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+---------+---------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# inspect out of range values\n",
    "spark.sql(\"\"\"\n",
    "            SELECT *\n",
    "            FROM data_init_view\n",
    "            WHERE pickup_datetime < \"2019-01-01 00:00:00\"\n",
    "            ORDER BY pickup_datetime DESC\n",
    "        \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count high out of range values\n",
    "spark.sql(\"\"\"\n",
    "            SELECT *\n",
    "            FROM data_init_view\n",
    "            WHERE pickup_datetime > \"2019-01-31 23:59:59\" \n",
    "        \"\"\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "441"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count low out of range values\n",
    "spark.sql(\"\"\"\n",
    "            SELECT *\n",
    "            FROM data_init_view\n",
    "            WHERE pickup_datetime < \"2019-01-01 00:00:00\"\n",
    "        \"\"\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "316"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "            SELECT *\n",
    "            FROM data_init_view\n",
    "            WHERE dropoff_datetime < \"2019-01-01 00:00:00\"\n",
    "        \"\"\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3011"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "            SELECT *\n",
    "            FROM data_init_view\n",
    "            WHERE dropoff_datetime > \"2019-01-31 23:59:59\" \n",
    "            ORDER BY dropoff_datetime\n",
    "        \"\"\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2915"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# investigate nye values\n",
    "spark.sql(\"\"\"\n",
    "            SELECT *\n",
    "            FROM data_init_view\n",
    "            WHERE dropoff_datetime > \"2019-01-31 23:59:59\"\n",
    "            AND pickup_datetime < \"2019-01-31 23:59:59\" \n",
    "            ORDER BY dropoff_datetime\n",
    "        \"\"\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+----------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+---------+---------+------+\n",
      "|VendorID|pickup_datetime|dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|trip_type|ehail_fee|colour|\n",
      "+--------+---------------+----------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+---------+---------+------+\n",
      "+--------+---------------+----------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+---------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show a candiate value to evaluate\n",
    "spark.sql(\"\"\"\n",
    "            SELECT *\n",
    "            FROM data_init_view\n",
    "            WHERE dropoff_datetime > \"2020-01-01 23:59:59\"\n",
    "            AND pickup_datetime < \"2019-12-31 23:59:59\" \n",
    "            ORDER BY dropoff_datetime\n",
    "        \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PU and DO Location containing 264 and 265 Location\n",
    "- PU\n",
    "    - 163631 with PU Location 264 and 265\n",
    "- DO\n",
    "    - 165911 with DO Location 264 and 265\n",
    "- 264\n",
    "    - 138614 where both are 264\n",
    "- 265\n",
    "    - 3034 where both are 265"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+\n",
      "|PULocationID| count|\n",
      "+------------+------+\n",
      "|         264|159760|\n",
      "|         265|  3871|\n",
      "+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# count where pickup location was unknown\n",
    "spark.sql(\"\"\"\n",
    "            SELECT PULocationID, count(PULocationID) AS count\n",
    "            FROM data_init_view\n",
    "            WHERE PULocationID = \"264\"\n",
    "            OR PULocationID = \"265\"\n",
    "            GROUP by PULocationID\n",
    "            ORDER BY count DESC\n",
    "        \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+\n",
      "|DOLocationID| count|\n",
      "+------------+------+\n",
      "|         264|149094|\n",
      "|         265| 16817|\n",
      "+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# and for dropoff\n",
    "spark.sql(\"\"\"\n",
    "            SELECT DOLocationID, count(DOLocationID) AS count\n",
    "            FROM data_init_view\n",
    "            WHERE DOLocationID = \"264\"\n",
    "            OR DOLocationID = \"265\"\n",
    "            GROUP by DOLocationID\n",
    "            ORDER BY count DESC\n",
    "        \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+\n",
      "|DOLocationID| count|\n",
      "+------------+------+\n",
      "|         264|138614|\n",
      "+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# count where pu location was 264 unknown as was dropoff\n",
    "spark.sql(\"\"\"\n",
    "            SELECT DOLocationID, count(DOLocationID) AS count\n",
    "            FROM data_init_view\n",
    "            WHERE PULocationID = \"264\"\n",
    "            AND DOLocationID = \"264\"\n",
    "            GROUP by DOLocationID\n",
    "            ORDER BY count DESC\n",
    "        \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|DOLocationID|count|\n",
      "+------------+-----+\n",
      "|         265| 3034|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# count where pu location was 264 unknown as was dropoff\n",
    "spark.sql(\"\"\"\n",
    "            SELECT DOLocationID, count(DOLocationID) AS count\n",
    "            FROM data_init_view\n",
    "            WHERE PULocationID = \"265\"\n",
    "            AND DOLocationID = \"265\"\n",
    "            GROUP by DOLocationID\n",
    "            ORDER BY count DESC\n",
    "        \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "811"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# where pickup was other unknown but dropoff was not\n",
    "spark.sql(\"\"\"\n",
    "            SELECT *\n",
    "            FROM data_init_view\n",
    "            WHERE PULocationID = \"265\"\n",
    "            AND DOLocationID != \"264\"\n",
    "            AND DOLocationID != \"265\"\n",
    "        \"\"\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13316"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# where dropoff was  unknown but pickup was not\n",
    "spark.sql(\"\"\"\n",
    "            SELECT *\n",
    "            FROM data_init_view\n",
    "            WHERE DOLocationID = \"265\"\n",
    "            AND PULocationID != \"264\"\n",
    "            AND PULocationID != \"265\"\n",
    "        \"\"\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10454"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "            SELECT *\n",
    "            FROM data_init_view\n",
    "            WHERE DOLocationID = \"264\"\n",
    "            AND PULocationID != \"264\"\n",
    "            AND PULocationID != \"265\"\n",
    "        \"\"\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3034"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "            SELECT *\n",
    "            FROM data_init_view\n",
    "            WHERE DOLocationID = \"265\"\n",
    "            AND PULocationID = \"265\"\n",
    "        \"\"\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: string (nullable = true)\n",
      " |-- pickup_datetime: string (nullable = true)\n",
      " |-- dropoff_datetime: string (nullable = true)\n",
      " |-- passenger_count: string (nullable = true)\n",
      " |-- trip_distance: string (nullable = true)\n",
      " |-- RatecodeID: string (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: string (nullable = true)\n",
      " |-- DOLocationID: string (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- fare_amount: string (nullable = true)\n",
      " |-- extra: string (nullable = true)\n",
      " |-- mta_tax: string (nullable = true)\n",
      " |-- tip_amount: string (nullable = true)\n",
      " |-- tolls_amount: string (nullable = true)\n",
      " |-- improvement_surcharge: string (nullable = true)\n",
      " |-- total_amount: string (nullable = true)\n",
      " |-- congestion_surcharge: string (nullable = true)\n",
      " |-- trip_type: string (nullable = false)\n",
      " |-- ehail_fee: string (nullable = false)\n",
      " |-- colour: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.dropTempView(\"data_init_view\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataunique = data.distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7667792"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataU = dataunique\n",
    "dataU.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Cleaning\n",
    "Duplicates?\n",
    "    - Run Distinct\n",
    "### Outcomes from Initial EDA\n",
    "1. Drop ehail_fee\n",
    "2. Drop ratecode=99\n",
    "3. Convert store_and_fwd_flag to bool\n",
    "4. Drop bad dates\n",
    "    - Low pickup_datetime<2019-01-01 00:00:00\n",
    "    - High pickup_datetime>2019-01-31 23:59:59\n",
    "    - Low dropoff_datetime<2019-01-01 00:00:00\n",
    "    - High dropoff_datetime>2019-01-31 23:59:59\n",
    "### Datatypes \n",
    "  - Initially all Strings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataU.createOrReplaceTempView(\"data_u_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "            CREATE OR REPLACE TEMP VIEW clean AS (\n",
    "                SELECT *\n",
    "                FROM data_u_view\n",
    "                WHERE RatecodeID != \"99\"\n",
    "                AND trip_type IS NOT NULL\n",
    "                AND PULocationID != \"265\"\n",
    "                AND pickup_datetime BETWEEN \"2019-01-01 00:00:00\" AND \"2019-01-31 23:59:59\"\n",
    "                AND dropoff_datetime BETWEEN \"2019-01-01 00:00:00\" AND \"2019-01-02 23:59:59\"\n",
    "            )\n",
    "        \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"       \n",
    "            CREATE OR REPLACE TEMP VIEW clean2 AS (\n",
    "            SELECT VendorID, \n",
    "                pickup_datetime,\n",
    "                dropoff_datetime,\n",
    "                passenger_count,\n",
    "                trip_distance,\n",
    "                RatecodeID,\n",
    "                PULocationID,\n",
    "                DOLocationID,\n",
    "                payment_type,\n",
    "                fare_amount,\n",
    "                extra,\n",
    "                mta_tax,\n",
    "                tip_amount,\n",
    "                tolls_amount,\n",
    "                improvement_surcharge,\n",
    "                total_amount,\n",
    "            CASE WHEN store_and_fwd_flag = \"Y\" THEN \"1\"\n",
    "            ELSE \"0\"\n",
    "            END AS store_and_fwd_flag,\n",
    "            CASE WHEN trip_type = \"1\" THEN \"0\"\n",
    "            ELSE \"0\"\n",
    "            END AS dispatched,\n",
    "            CASE WHEN colour = \"yellow\" THEN \"0\"\n",
    "            ELSE \"0\"\n",
    "            END AS colour\n",
    "            FROM clean\n",
    "            )\n",
    "        \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataC = spark.sql(\"SELECT * FROM clean2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['VendorID',\n",
       " 'pickup_datetime',\n",
       " 'dropoff_datetime',\n",
       " 'passenger_count',\n",
       " 'trip_distance',\n",
       " 'RatecodeID',\n",
       " 'PULocationID',\n",
       " 'DOLocationID',\n",
       " 'payment_type',\n",
       " 'fare_amount',\n",
       " 'extra',\n",
       " 'mta_tax',\n",
       " 'tip_amount',\n",
       " 'tolls_amount',\n",
       " 'improvement_surcharge',\n",
       " 'total_amount',\n",
       " 'store_and_fwd_flag',\n",
       " 'dispatched',\n",
       " 'colour']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataC.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleck all columns for NA/Null\n",
    "- NA\n",
    "- N/A\n",
    "- NAN\n",
    "- NULL\n",
    "- NIL\n",
    "- \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"SELECT NULLIF(UPPER(VendorID), UPPER('NA')) AS VendorID, NULLIF(UPPER(pickup_datetime), UPPER('NA')) AS pickup_datetime, NULLIF(UPPER(dropoff_datetime), UPPER('NA')) AS dropoff_datetime, NULLIF(UPPER(passenger_count), UPPER('NA')) AS passenger_count, NULLIF(UPPER(trip_distance), UPPER('NA')) AS trip_distance, NULLIF(UPPER(RatecodeID), UPPER('NA')) AS RatecodeID, NULLIF(UPPER(PULocationID), UPPER('NA')) AS PULocationID, NULLIF(UPPER(DOLocationID), UPPER('NA')) AS DOLocationID, NULLIF(UPPER(payment_type), UPPER('NA')) AS payment_type, NULLIF(UPPER(fare_amount), UPPER('NA')) AS fare_amount, NULLIF(UPPER(extra), UPPER('NA')) AS extra, NULLIF(UPPER(mta_tax), UPPER('NA')) AS mta_tax, NULLIF(UPPER(tip_amount), UPPER('NA')) AS tip_amount, NULLIF(UPPER(tolls_amount), UPPER('NA')) AS tolls_amount, NULLIF(UPPER(improvement_surcharge), UPPER('NA')) AS improvement_surcharge, NULLIF(UPPER(total_amount), UPPER('NA')) AS total_amount, NULLIF(UPPER(store_and_fwd_flag), UPPER('NA')) AS store_and_fwd_flag, NULLIF(UPPER(dispatched), UPPER('NA')) AS dispatched, NULLIF(UPPER(colour), UPPER('NA')) AS colour FROM clean2\",\n",
       " \"SELECT NULLIF(UPPER(VendorID), UPPER('N/A')) AS VendorID, NULLIF(UPPER(pickup_datetime), UPPER('N/A')) AS pickup_datetime, NULLIF(UPPER(dropoff_datetime), UPPER('N/A')) AS dropoff_datetime, NULLIF(UPPER(passenger_count), UPPER('N/A')) AS passenger_count, NULLIF(UPPER(trip_distance), UPPER('N/A')) AS trip_distance, NULLIF(UPPER(RatecodeID), UPPER('N/A')) AS RatecodeID, NULLIF(UPPER(PULocationID), UPPER('N/A')) AS PULocationID, NULLIF(UPPER(DOLocationID), UPPER('N/A')) AS DOLocationID, NULLIF(UPPER(payment_type), UPPER('N/A')) AS payment_type, NULLIF(UPPER(fare_amount), UPPER('N/A')) AS fare_amount, NULLIF(UPPER(extra), UPPER('N/A')) AS extra, NULLIF(UPPER(mta_tax), UPPER('N/A')) AS mta_tax, NULLIF(UPPER(tip_amount), UPPER('N/A')) AS tip_amount, NULLIF(UPPER(tolls_amount), UPPER('N/A')) AS tolls_amount, NULLIF(UPPER(improvement_surcharge), UPPER('N/A')) AS improvement_surcharge, NULLIF(UPPER(total_amount), UPPER('N/A')) AS total_amount, NULLIF(UPPER(store_and_fwd_flag), UPPER('N/A')) AS store_and_fwd_flag, NULLIF(UPPER(dispatched), UPPER('N/A')) AS dispatched, NULLIF(UPPER(colour), UPPER('N/A')) AS colour FROM clean2\",\n",
       " \"SELECT NULLIF(UPPER(VendorID), UPPER('NAN')) AS VendorID, NULLIF(UPPER(pickup_datetime), UPPER('NAN')) AS pickup_datetime, NULLIF(UPPER(dropoff_datetime), UPPER('NAN')) AS dropoff_datetime, NULLIF(UPPER(passenger_count), UPPER('NAN')) AS passenger_count, NULLIF(UPPER(trip_distance), UPPER('NAN')) AS trip_distance, NULLIF(UPPER(RatecodeID), UPPER('NAN')) AS RatecodeID, NULLIF(UPPER(PULocationID), UPPER('NAN')) AS PULocationID, NULLIF(UPPER(DOLocationID), UPPER('NAN')) AS DOLocationID, NULLIF(UPPER(payment_type), UPPER('NAN')) AS payment_type, NULLIF(UPPER(fare_amount), UPPER('NAN')) AS fare_amount, NULLIF(UPPER(extra), UPPER('NAN')) AS extra, NULLIF(UPPER(mta_tax), UPPER('NAN')) AS mta_tax, NULLIF(UPPER(tip_amount), UPPER('NAN')) AS tip_amount, NULLIF(UPPER(tolls_amount), UPPER('NAN')) AS tolls_amount, NULLIF(UPPER(improvement_surcharge), UPPER('NAN')) AS improvement_surcharge, NULLIF(UPPER(total_amount), UPPER('NAN')) AS total_amount, NULLIF(UPPER(store_and_fwd_flag), UPPER('NAN')) AS store_and_fwd_flag, NULLIF(UPPER(dispatched), UPPER('NAN')) AS dispatched, NULLIF(UPPER(colour), UPPER('NAN')) AS colour FROM clean2\",\n",
       " \"SELECT NULLIF(UPPER(VendorID), UPPER('NIL')) AS VendorID, NULLIF(UPPER(pickup_datetime), UPPER('NIL')) AS pickup_datetime, NULLIF(UPPER(dropoff_datetime), UPPER('NIL')) AS dropoff_datetime, NULLIF(UPPER(passenger_count), UPPER('NIL')) AS passenger_count, NULLIF(UPPER(trip_distance), UPPER('NIL')) AS trip_distance, NULLIF(UPPER(RatecodeID), UPPER('NIL')) AS RatecodeID, NULLIF(UPPER(PULocationID), UPPER('NIL')) AS PULocationID, NULLIF(UPPER(DOLocationID), UPPER('NIL')) AS DOLocationID, NULLIF(UPPER(payment_type), UPPER('NIL')) AS payment_type, NULLIF(UPPER(fare_amount), UPPER('NIL')) AS fare_amount, NULLIF(UPPER(extra), UPPER('NIL')) AS extra, NULLIF(UPPER(mta_tax), UPPER('NIL')) AS mta_tax, NULLIF(UPPER(tip_amount), UPPER('NIL')) AS tip_amount, NULLIF(UPPER(tolls_amount), UPPER('NIL')) AS tolls_amount, NULLIF(UPPER(improvement_surcharge), UPPER('NIL')) AS improvement_surcharge, NULLIF(UPPER(total_amount), UPPER('NIL')) AS total_amount, NULLIF(UPPER(store_and_fwd_flag), UPPER('NIL')) AS store_and_fwd_flag, NULLIF(UPPER(dispatched), UPPER('NIL')) AS dispatched, NULLIF(UPPER(colour), UPPER('NIL')) AS colour FROM clean2\",\n",
       " \"SELECT NULLIF(UPPER(VendorID), UPPER('NULL')) AS VendorID, NULLIF(UPPER(pickup_datetime), UPPER('NULL')) AS pickup_datetime, NULLIF(UPPER(dropoff_datetime), UPPER('NULL')) AS dropoff_datetime, NULLIF(UPPER(passenger_count), UPPER('NULL')) AS passenger_count, NULLIF(UPPER(trip_distance), UPPER('NULL')) AS trip_distance, NULLIF(UPPER(RatecodeID), UPPER('NULL')) AS RatecodeID, NULLIF(UPPER(PULocationID), UPPER('NULL')) AS PULocationID, NULLIF(UPPER(DOLocationID), UPPER('NULL')) AS DOLocationID, NULLIF(UPPER(payment_type), UPPER('NULL')) AS payment_type, NULLIF(UPPER(fare_amount), UPPER('NULL')) AS fare_amount, NULLIF(UPPER(extra), UPPER('NULL')) AS extra, NULLIF(UPPER(mta_tax), UPPER('NULL')) AS mta_tax, NULLIF(UPPER(tip_amount), UPPER('NULL')) AS tip_amount, NULLIF(UPPER(tolls_amount), UPPER('NULL')) AS tolls_amount, NULLIF(UPPER(improvement_surcharge), UPPER('NULL')) AS improvement_surcharge, NULLIF(UPPER(total_amount), UPPER('NULL')) AS total_amount, NULLIF(UPPER(store_and_fwd_flag), UPPER('NULL')) AS store_and_fwd_flag, NULLIF(UPPER(dispatched), UPPER('NULL')) AS dispatched, NULLIF(UPPER(colour), UPPER('NULL')) AS colour FROM clean2\",\n",
       " \"SELECT NULLIF(UPPER(VendorID), UPPER(' ')) AS VendorID, NULLIF(UPPER(pickup_datetime), UPPER(' ')) AS pickup_datetime, NULLIF(UPPER(dropoff_datetime), UPPER(' ')) AS dropoff_datetime, NULLIF(UPPER(passenger_count), UPPER(' ')) AS passenger_count, NULLIF(UPPER(trip_distance), UPPER(' ')) AS trip_distance, NULLIF(UPPER(RatecodeID), UPPER(' ')) AS RatecodeID, NULLIF(UPPER(PULocationID), UPPER(' ')) AS PULocationID, NULLIF(UPPER(DOLocationID), UPPER(' ')) AS DOLocationID, NULLIF(UPPER(payment_type), UPPER(' ')) AS payment_type, NULLIF(UPPER(fare_amount), UPPER(' ')) AS fare_amount, NULLIF(UPPER(extra), UPPER(' ')) AS extra, NULLIF(UPPER(mta_tax), UPPER(' ')) AS mta_tax, NULLIF(UPPER(tip_amount), UPPER(' ')) AS tip_amount, NULLIF(UPPER(tolls_amount), UPPER(' ')) AS tolls_amount, NULLIF(UPPER(improvement_surcharge), UPPER(' ')) AS improvement_surcharge, NULLIF(UPPER(total_amount), UPPER(' ')) AS total_amount, NULLIF(UPPER(store_and_fwd_flag), UPPER(' ')) AS store_and_fwd_flag, NULLIF(UPPER(dispatched), UPPER(' ')) AS dispatched, NULLIF(UPPER(colour), UPPER(' ')) AS colour FROM clean2\"]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# common na strings as list\n",
    "commonNA = [\"NA\", \"N/A\", \"NAN\", \"NIL\", \"NULL\", \" \"]\n",
    "\n",
    "# columns from dataset\n",
    "columns = dataC.columns\n",
    "\n",
    "# blank output list\n",
    "anil = []\n",
    "\n",
    "# write a sql query string that converts any in the common na list in all columns in columns list to null using SQL NULLIF\n",
    "# first by na string\n",
    "for nval in commonNA:\n",
    "    scol = []\n",
    "    for col in columns:\n",
    "        # constrcuts string for each column\n",
    "        nif = f\"NULLIF(UPPER({col}), UPPER('{nval}')) AS {col}\"\n",
    "        scol.append(nif)\n",
    "    scol = \", \".join(scol)\n",
    "    # puts all null strings into select statement\n",
    "    scol = f\"SELECT {scol} FROM clean2\"\n",
    "    anil.append(scol)\n",
    "\n",
    "# check output\n",
    "anil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a query for each na string, replaceing view each update\n",
    "for query in anil:\n",
    "    fq = f\"CREATE OR REPLACE TEMP VIEW clean2 AS ({query})\"\n",
    "    spark.sql(fq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to df\n",
    "dataC = spark.sql(\"SELECT * FROM clean2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['VendorID',\n",
       " 'pickup_datetime',\n",
       " 'dropoff_datetime',\n",
       " 'passenger_count',\n",
       " 'trip_distance',\n",
       " 'RatecodeID',\n",
       " 'PULocationID',\n",
       " 'DOLocationID',\n",
       " 'payment_type',\n",
       " 'fare_amount',\n",
       " 'extra',\n",
       " 'mta_tax',\n",
       " 'tip_amount',\n",
       " 'tolls_amount',\n",
       " 'improvement_surcharge',\n",
       " 'total_amount',\n",
       " 'store_and_fwd_flag',\n",
       " 'dispatched',\n",
       " 'colour']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataC.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataC3 = dataC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataC3 = spark.read.parquet(\"./data/clean2.parquet\")\n",
    "dataC3.createOrReplaceTempView(\"data_c3_view\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trip Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "            CREATE OR REPLACE TEMP VIEW trip_view AS (\n",
    "            SELECT *, datediff(dropoff_datetime, pickup_datetime) as tripdays\n",
    "            FROM data_c3_view\n",
    "            )\n",
    "        \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "tripClean = spark.sql(\"\"\"\n",
    "            SELECT *\n",
    "            FROM trip_view\n",
    "            WHERE tripdays <= 1\n",
    "        \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refunds\n",
    "- Initially used just pickup datetime, dropoff datetime and fare_amount needs to include pickup and dropoff location and use total amount\n",
    "    - no more triples\n",
    "    - 420\n",
    "- refund flag and remove duplicate record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTC = tripClean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataTC = dataTC.repartition(180) # increase from 36 due to shuffle spill\n",
    "dataTC.createOrReplaceTempView(\"data_TC_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "420"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count number of potential refunds\n",
    "# use absolute to match negative totals with positives\n",
    "spark.sql(\"\"\"\n",
    "            SELECT pickup_datetime, dropoff_datetime, PULocationID, DOLocationID, ABS(total_amount) AS fare, count(*) AS count\n",
    "            FROM data_TC_view\n",
    "            GROUP BY pickup_datetime, dropoff_datetime, PULocationID, DOLocationID, fare\n",
    "            HAVING count > 1\n",
    "            ORDER BY count DESC\n",
    "            \"\"\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+------------+------------+----+-----+\n",
      "|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|fare|count|\n",
      "+-------------------+-------------------+------------+------------+----+-----+\n",
      "|2019-01-02 12:51:51|2019-01-02 12:52:05|          93|          93| 3.8|    2|\n",
      "|2019-01-01 18:38:10|2019-01-01 18:39:35|          75|          75|52.8|    2|\n",
      "|2019-01-02 13:21:02|2019-01-02 13:24:38|         236|         239| 5.3|    2|\n",
      "|2019-01-02 15:49:13|2019-01-02 15:55:43|         237|         238| 7.8|    2|\n",
      "|2019-01-01 00:33:02|2019-01-01 00:36:38|         166|         151| 6.3|    2|\n",
      "|2019-01-01 16:18:12|2019-01-01 16:30:03|         144|         158|10.8|    2|\n",
      "|2019-01-01 14:48:32|2019-01-01 14:52:57|         209|         261| 5.3|    2|\n",
      "|2019-01-01 04:50:18|2019-01-01 04:50:25|         264|         235|80.3|    2|\n",
      "|2019-01-01 18:25:16|2019-01-01 18:25:24|         142|          43|52.8|    2|\n",
      "|2019-01-02 17:32:52|2019-01-02 17:38:31|         231|          45| 7.8|    2|\n",
      "|2019-01-01 05:13:22|2019-01-01 05:15:26|          79|         113| 4.8|    2|\n",
      "|2019-01-01 04:23:42|2019-01-01 04:26:52|          75|          75| 5.8|    2|\n",
      "|2019-01-01 15:43:08|2019-01-01 15:44:43|         162|         170| 4.3|    2|\n",
      "|2019-01-02 20:25:48|2019-01-02 20:30:17|         211|         209| 6.3|    2|\n",
      "|2019-01-01 12:35:38|2019-01-01 12:36:27|         236|         236| 3.8|    2|\n",
      "|2019-01-02 12:29:18|2019-01-02 12:33:55|         238|         238| 5.3|    2|\n",
      "|2019-01-01 15:46:07|2019-01-01 15:50:44|         186|         234| 6.3|    2|\n",
      "|2019-01-01 14:08:31|2019-01-01 14:08:39|         237|          43| 3.3|    2|\n",
      "|2019-01-02 22:51:18|2019-01-02 22:52:06|         138|         138| 3.8|    2|\n",
      "|2019-01-01 12:18:29|2019-01-01 12:19:35|         211|         211| 3.8|    2|\n",
      "+-------------------+-------------------+------------+------------+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show examples\n",
    "spark.sql(\"\"\"\n",
    "            SELECT pickup_datetime, dropoff_datetime, PULocationID, DOLocationID, ABS(total_amount) AS fare, count(*) AS count\n",
    "            FROM data_TC_view\n",
    "            GROUP BY pickup_datetime, dropoff_datetime, PULocationID, DOLocationID, fare\n",
    "            HAVING count > 1\n",
    "            ORDER BY count DESC\n",
    "            \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create view redistributing data to reduce later shuffle\n",
    "spark.sql(\"\"\"\n",
    "            CREATE OR REPLACE TEMP VIEW data_TC_redis AS\n",
    "            SELECT *\n",
    "            FROM data_TC_view\n",
    "            DISTRIBUTE BY PULocationID\n",
    "            SORT BY DOLocationID, pickup_datetime, dropoff_datetime\n",
    "            \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create view of duplicates / refunds\n",
    "spark.sql(\"\"\"\n",
    "            CREATE OR REPLACE TEMP VIEW refunds AS\n",
    "            SELECT pickup_datetime, dropoff_datetime, PULocationID, DOLocationID, ABS(total_amount) AS fare, count(*) AS count\n",
    "            FROM data_TC_view\n",
    "            GROUP BY PULocationID, DOLocationID, pickup_datetime, dropoff_datetime, fare\n",
    "            HAVING count > 1\n",
    "            DISTRIBUTE BY PULocationID\n",
    "            SORT BY DOLocationID, pickup_datetime, dropoff_datetime\n",
    "            \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create refund flag, when multiple flag is 1\n",
    "refunds = spark.sql(\"\"\"\n",
    "            SELECT data_tc_redis.*, CASE WHEN refunds.count = \"2\" THEN \"1\"\n",
    "                                    ELSE \"0\"\n",
    "                                    END AS refunded_flag\n",
    "            FROM data_tc_redis\n",
    "            LEFT JOIN refunds\n",
    "            ON data_tc_redis.PULocationID = refunds.PULocationID\n",
    "            AND data_tc_redis.DOLocationID = refunds.DOLocationID\n",
    "            AND data_tc_redis.pickup_datetime = refunds.pickup_datetime\n",
    "            AND data_tc_redis.dropoff_datetime = refunds.dropoff_datetime\n",
    "            \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataR = refunds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataR = spark.read.parquet(\"./data/refundFlagged.parquet\")\n",
    "dataR.createOrReplaceTempView(\"data_R_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+\n",
      "|refunded_flag|count(refunded_flag)|\n",
      "+-------------+--------------------+\n",
      "|            0|              384221|\n",
      "|            1|                 840|\n",
      "+-------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check number of \"refunds\"\n",
    "spark.sql(\"\"\"\n",
    "            SELECT refunded_flag, count(refunded_flag)\n",
    "            FROM data_R_view\n",
    "            GROUP by refunded_flag\n",
    "        \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-------------------+---------------+-------------+----------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+------------------+----------+------+--------+-------------+\n",
      "|VendorID|    pickup_datetime|   dropoff_datetime|passenger_count|trip_distance|RatecodeID|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|store_and_fwd_flag|dispatched|colour|tripdays|refunded_flag|\n",
      "+--------+-------------------+-------------------+---------------+-------------+----------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+------------------+----------+------+--------+-------------+\n",
      "|       2|2019-01-02 19:40:58|2019-01-02 19:44:44|              1|          .51|         1|         237|         141|           2|        4.5|    1|    0.5|         0|           1|                  0.3|         7.3|                 0|         0|     0|       0|            1|\n",
      "|       2|2019-01-02 19:40:58|2019-01-02 19:44:44|              1|          .51|         1|         237|         141|           3|       -4.5|   -1|   -0.5|         0|          -1|                 -0.3|        -7.3|                 0|         0|     0|       0|            1|\n",
      "|       2|2019-01-02 11:21:20|2019-01-02 11:23:36|              1|          .35|         1|         161|         161|           2|        3.5|    0|    0.5|         0|           0|                  0.3|         4.3|                 0|         0|     0|       0|            1|\n",
      "|       2|2019-01-02 11:21:20|2019-01-02 11:23:36|              1|          .35|         1|         161|         161|           3|       -3.5|    0|   -0.5|         0|           0|                 -0.3|        -4.3|                 0|         0|     0|       0|            1|\n",
      "|       2|2019-01-01 03:03:44|2019-01-01 03:04:15|              1|          .02|         1|         226|         226|           2|        2.5|  0.5|    0.5|         0|           0|                  0.3|         3.8|                 0|         0|     0|       0|            1|\n",
      "|       2|2019-01-01 03:03:44|2019-01-01 03:04:15|              1|          .02|         1|         226|         226|           4|       -2.5| -0.5|   -0.5|         0|           0|                 -0.3|        -3.8|                 0|         0|     0|       0|            1|\n",
      "|       2|2019-01-02 12:29:18|2019-01-02 12:33:55|              2|          .03|         1|         238|         238|           4|       -4.5|    0|   -0.5|         0|           0|                 -0.3|        -5.3|                 0|         0|     0|       0|            1|\n",
      "|       2|2019-01-02 12:29:18|2019-01-02 12:33:55|              2|          .03|         1|         238|         238|           2|        4.5|    0|    0.5|         0|           0|                  0.3|         5.3|                 0|         0|     0|       0|            1|\n",
      "|       2|2019-01-02 18:48:28|2019-01-02 18:51:30|              1|          .00|         1|           7|         193|           2|        2.5|    1|    0.5|         0|           0|                  0.3|         4.3|                 0|         0|     0|       0|            1|\n",
      "|       2|2019-01-02 18:48:28|2019-01-02 18:51:30|              1|          .00|         1|           7|         193|           3|       -2.5|   -1|   -0.5|         0|           0|                 -0.3|        -4.3|                 0|         0|     0|       0|            1|\n",
      "|       2|2019-01-02 08:24:02|2019-01-02 08:24:11|              1|          .00|         3|         162|         162|           4|        -20|    0|      0|         0|           0|                 -0.3|       -20.3|                 0|         0|     0|       0|            1|\n",
      "|       2|2019-01-02 08:24:02|2019-01-02 08:24:11|              1|          .00|         3|         162|         162|           2|         20|    0|      0|         0|           0|                  0.3|        20.3|                 0|         0|     0|       0|            1|\n",
      "|       2|2019-01-02 07:26:03|2019-01-02 08:05:47|              1|        11.86|         1|         186|          14|           2|        -40|    0|   -0.5|         0|           0|                 -0.3|       -40.8|                 0|         0|     0|       0|            1|\n",
      "|       2|2019-01-02 07:26:03|2019-01-02 08:05:47|              1|        11.86|         1|         186|          14|           2|         40|    0|    0.5|         0|           0|                  0.3|        40.8|                 0|         0|     0|       0|            1|\n",
      "|       2|2019-01-02 16:11:12|2019-01-02 16:16:30|              1|          .97|         1|         161|         234|           2|        5.5|    1|    0.5|         0|           0|                  0.3|         7.3|                 0|         0|     0|       0|            1|\n",
      "|       2|2019-01-02 16:11:12|2019-01-02 16:16:30|              1|          .97|         1|         161|         234|           4|       -5.5|   -1|   -0.5|         0|           0|                 -0.3|        -7.3|                 0|         0|     0|       0|            1|\n",
      "|       2|2019-01-01 00:51:47|2019-01-01 00:53:44|              1|          .36|         1|         186|         186|           2|        3.5|  0.5|    0.5|         0|           0|                  0.3|         4.8|                 0|         0|     0|       0|            1|\n",
      "|       2|2019-01-01 00:51:47|2019-01-01 00:53:44|              1|          .36|         1|         186|         186|           4|       -3.5| -0.5|   -0.5|         0|           0|                 -0.3|        -4.8|                 0|         0|     0|       0|            1|\n",
      "|       2|2019-01-02 14:53:24|2019-01-02 14:55:19|              1|          .14|         1|          43|          43|           4|       -3.5|    0|   -0.5|         0|           0|                 -0.3|        -4.3|                 0|         0|     0|       0|            1|\n",
      "|       2|2019-01-02 14:53:24|2019-01-02 14:55:19|              1|          .14|         1|          43|          43|           2|        3.5|    0|    0.5|         0|           0|                  0.3|         4.3|                 0|         0|     0|       0|            1|\n",
      "+--------+-------------------+-------------------+---------------+-------------+----------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+------------------+----------+------+--------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# inspect examples\n",
    "spark.sql(\"\"\"\n",
    "            SELECT *\n",
    "            FROM data_R_view\n",
    "            WHERE refunded_flag = 1\n",
    "            \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count where total amount is 0\n",
    "spark.sql(\"\"\"\n",
    "            SELECT *\n",
    "            FROM data_R_view\n",
    "            WHERE refunded_flag = 1\n",
    "            AND NOT total_amount < \"0\"\n",
    "            AND NOT total_amount > \"0\"           \n",
    "            \"\"\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some equal fares so need to investigate\n",
    "equalfares = spark.sql(\"\"\"\n",
    "                        SELECT pickup_datetime, dropoff_datetime, PULocationID, DOLocationID, total_amount, count(*) AS count\n",
    "                        FROM data_R_view\n",
    "                        GROUP BY PULocationID, DOLocationID, pickup_datetime, dropoff_datetime, total_amount\n",
    "                        HAVING count > 1\n",
    "                        \"\"\")\n",
    "# equalfares.cache()\n",
    "equalfares.createOrReplaceTempView(\"dups_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "equalfares.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use left join exlusive to remove furhter duplicates\n",
    "removedups = spark.sql(\"\"\"\n",
    "            SELECT data_R_view.*\n",
    "            FROM data_R_view\n",
    "            LEFT JOIN dups_view\n",
    "            ON data_R_view.PULocationID = dups_view.PULocationID\n",
    "            AND data_R_view.DOLocationID = dups_view.DOLocationID\n",
    "            AND data_R_view.pickup_datetime = dups_view.pickup_datetime\n",
    "            AND data_R_view.dropoff_datetime = dups_view.dropoff_datetime\n",
    "            WHERE dups_view.PULocationID IS NULL\n",
    "            AND dups_view.DOLocationID IS NULL\n",
    "            AND dups_view.pickup_datetime IS NULL\n",
    "            AND dups_view.dropoff_datetime IS NULL\n",
    "            \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "datarfd = removedups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "datarfd.createOrReplaceTempView(\"rem_dups_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------------+------------+------------+------------+-----+\n",
      "|pickup_datetime|dropoff_datetime|PULocationID|DOLocationID|total_amount|count|\n",
      "+---------------+----------------+------------+------------+------------+-----+\n",
      "+---------------+----------------+------------+------------+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "            SELECT pickup_datetime, dropoff_datetime, PULocationID, DOLocationID, total_amount, count(*) AS count\n",
    "            FROM rem_dups_view\n",
    "            GROUP BY PULocationID, DOLocationID, pickup_datetime, dropoff_datetime, total_amount\n",
    "            HAVING count > 1\n",
    "            \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "removeRefund = spark.sql(\"\"\"\n",
    "            SELECT *\n",
    "            FROM rem_dups_view\n",
    "            WHERE NOT (refunded_flag = \"1\" AND total_amount < \"0\")\n",
    "            \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384641"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check impact\n",
    "removeRefund.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check negative values again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datarfd = spark.read.parquet(\"./data/refunds-cleaned.parquet\")\n",
    "datarfd = removeRefund\n",
    "datarfd.createOrReplaceTempView(\"ref_clean_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|extra| count|\n",
      "+-----+------+\n",
      "|    0|204848|\n",
      "|  0.5|126463|\n",
      "|    1| 50785|\n",
      "|  4.5|  2486|\n",
      "| -0.5|    12|\n",
      "|  2.5|    11|\n",
      "|  0.8|    10|\n",
      "| 17.5|     9|\n",
      "|    3|     5|\n",
      "|  1.3|     3|\n",
      "|  1.8|     2|\n",
      "| 18.5|     2|\n",
      "|   -1|     1|\n",
      "|   18|     1|\n",
      "|  3.5|     1|\n",
      "|  5.3|     1|\n",
      "| -4.5|     1|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# extra\n",
    "spark.sql(\"\"\"\n",
    "            SELECT extra, count(extra) AS count\n",
    "            FROM ref_clean_view\n",
    "            GROUP by extra\n",
    "            ORDER BY count DESC\n",
    "        \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|mta_tax| count|\n",
      "+-------+------+\n",
      "|    0.5|382200|\n",
      "|      0|  2417|\n",
      "|   -0.5|    24|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# mta_tax \n",
    "\n",
    "spark.sql(\"\"\"\n",
    "            SELECT mta_tax, count(mta_tax) AS count\n",
    "            FROM ref_clean_view\n",
    "            GROUP by mta_tax\n",
    "            ORDER BY count DESC\n",
    "        \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+------+\n",
      "|improvement_surcharge| count|\n",
      "+---------------------+------+\n",
      "|                  0.3|384455|\n",
      "|                    0|   160|\n",
      "|                 -0.3|    26|\n",
      "+---------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "            SELECT improvement_surcharge, count(improvement_surcharge) AS count\n",
    "            FROM ref_clean_view\n",
    "            GROUP by improvement_surcharge\n",
    "            ORDER BY count DESC\n",
    "        \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop low qty out of range values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "            CREATE OR REPLACE TEMP VIEW rem_one AS\n",
    "            SELECT *\n",
    "            FROM ref_clean_view\n",
    "            WHERE mta_tax = \"0.5\"\n",
    "            OR mta_tax = \"0\"\n",
    "        \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "            CREATE OR REPLACE TEMP VIEW rem_two AS\n",
    "            SELECT *\n",
    "            FROM rem_one\n",
    "            WHERE improvement_surcharge = \"0.3\"\n",
    "            OR improvement_surcharge = \"0\"\n",
    "        \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = spark.sql(\"\"\"\n",
    "            SELECT *\n",
    "            FROM rem_two\n",
    "        \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['VendorID',\n",
       " 'pickup_datetime',\n",
       " 'dropoff_datetime',\n",
       " 'passenger_count',\n",
       " 'trip_distance',\n",
       " 'RatecodeID',\n",
       " 'PULocationID',\n",
       " 'DOLocationID',\n",
       " 'payment_type',\n",
       " 'fare_amount',\n",
       " 'extra',\n",
       " 'mta_tax',\n",
       " 'tip_amount',\n",
       " 'tolls_amount',\n",
       " 'improvement_surcharge',\n",
       " 'total_amount',\n",
       " 'store_and_fwd_flag',\n",
       " 'dispatched',\n",
       " 'colour',\n",
       " 'tripdays',\n",
       " 'refunded_flag']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: string (nullable = true)\n",
      " |-- pickup_datetime: string (nullable = true)\n",
      " |-- dropoff_datetime: string (nullable = true)\n",
      " |-- passenger_count: string (nullable = true)\n",
      " |-- trip_distance: string (nullable = true)\n",
      " |-- RatecodeID: string (nullable = true)\n",
      " |-- PULocationID: string (nullable = true)\n",
      " |-- DOLocationID: string (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- fare_amount: string (nullable = true)\n",
      " |-- extra: string (nullable = true)\n",
      " |-- mta_tax: string (nullable = true)\n",
      " |-- tip_amount: string (nullable = true)\n",
      " |-- tolls_amount: string (nullable = true)\n",
      " |-- improvement_surcharge: string (nullable = true)\n",
      " |-- total_amount: string (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- dispatched: string (nullable = true)\n",
      " |-- colour: string (nullable = true)\n",
      " |-- tripdays: integer (nullable = true)\n",
      " |-- refunded_flag: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cols.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convert mta_tax and improvement_surchage to bools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "            CREATE OR REPLACE TEMP VIEW clean_one AS\n",
    "            SELECT VendorID,\n",
    "                    pickup_datetime,\n",
    "                    dropoff_datetime,\n",
    "                    passenger_count,\n",
    "                    trip_distance,\n",
    "                    RatecodeID,\n",
    "                    PULocationID,\n",
    "                    DOLocationID,\n",
    "                    payment_type,\n",
    "                    fare_amount,\n",
    "                    extra,\n",
    "                    tip_amount,\n",
    "                    tolls_amount,\n",
    "                    total_amount,\n",
    "                    store_and_fwd_flag,\n",
    "                    dispatched,\n",
    "                    colour,\n",
    "                    tripdays,\n",
    "                    refunded_flag,\n",
    "                    CASE WHEN mta_tax = \"0.5\" THEN \"1\"\n",
    "                    ELSE \"0\"\n",
    "                    END AS mta_tax,\n",
    "                    CASE WHEN improvement_surcharge = \"0.3\" THEN \"1\"\n",
    "                    ELSE \"0\"\n",
    "                    END AS improvement_surcharge\n",
    "            FROM rem_two\n",
    "        \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cleaner data enable use of smaller impact datatypes\n",
    "spark.sql(\"\"\"\n",
    "            CREATE OR REPLACE TEMP VIEW clean_two AS\n",
    "            SELECT TINYINT(VendorID),\n",
    "                        TIMESTAMP(pickup_datetime),\n",
    "                        TIMESTAMP(dropoff_datetime),\n",
    "                        TINYINT(passenger_count),\n",
    "                        FLOAT(trip_distance),\n",
    "                        TINYINT(RatecodeID),\n",
    "                        SMALLINT(PULocationID),\n",
    "                        SMALLINT(DOLocationID),\n",
    "                        TINYINT(payment_type),\n",
    "                        FLOAT(fare_amount),\n",
    "                        FLOAT(extra),\n",
    "                        FLOAT(tip_amount),\n",
    "                        FLOAT(tolls_amount),\n",
    "                        FLOAT(total_amount),\n",
    "                        BOOLEAN(store_and_fwd_flag),\n",
    "                        BOOLEAN(dispatched),\n",
    "                        BOOLEAN(colour) AS yellow,\n",
    "                        tripdays,\n",
    "                        BOOLEAN(refunded_flag),\n",
    "                        BOOLEAN(mta_tax),\n",
    "                        BOOLEAN(improvement_surcharge)\n",
    "            FROM clean_one\n",
    "        \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_clean = spark.sql(\"\"\"\n",
    "                            SELECT *\n",
    "                            FROM clean_two\n",
    "                        \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datainitclean = spark.read.parquet(\"./data/out_init_clean.parquet\")\n",
    "datainitclean = out_clean\n",
    "datainitclean.createOrReplaceTempView(\"init_clean_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# need to convert floats to decimals for arithmetic\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "            CREATE OR REPLACE TEMP VIEW ic_two_view AS\n",
    "            SELECT VendorID,\n",
    "                    pickup_datetime,\n",
    "                    dropoff_datetime,\n",
    "                    passenger_count,\n",
    "                    CAST(trip_distance AS DECIMAL(10,3)),\n",
    "                    RatecodeID,\n",
    "                    PULocationID,\n",
    "                    DOLocationID,\n",
    "                    payment_type,\n",
    "                    CAST(fare_amount AS DECIMAL(10,3)),\n",
    "                    CAST(extra AS DECIMAL(10,3)),\n",
    "                    CAST(tip_amount AS DECIMAL(10,3)),\n",
    "                    CAST(tolls_amount AS DECIMAL(10,3)),\n",
    "                    CAST(total_amount AS DECIMAL(10,3)),\n",
    "                    store_and_fwd_flag,\n",
    "                    dispatched,\n",
    "                    yellow,\n",
    "                    tripdays,\n",
    "                    refunded_flag,\n",
    "                    mta_tax,\n",
    "                    improvement_surcharge\n",
    "            FROM init_clean_view\n",
    "        \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|yellow| count|\n",
      "+------+------+\n",
      "| false|384615|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "            SELECT yellow, count(yellow) AS count\n",
    "            FROM ic_two_view\n",
    "            GROUP BY yellow\n",
    "            ORDER BY count DESC\n",
    "            \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total_amount investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+\n",
      "|totals_equal| count|\n",
      "+------------+------+\n",
      "|           1|383613|\n",
      "|           0|  1002|\n",
      "+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "            WITH calc AS (\n",
    "                WITH conv AS (\n",
    "                    SELECT fare_amount, extra, tip_amount, tolls_amount, total_amount,\n",
    "                        CASE WHEN mta_tax = true THEN 0.5 ELSE 0 END AS tax,\n",
    "                        CASE WHEN improvement_surcharge = true THEN 0.3 ELSE 0 END AS sur\n",
    "                    FROM ic_two_view\n",
    "                )\n",
    "                SELECT CASE WHEN (fare_amount +\n",
    "                    extra +\n",
    "                    tax +\n",
    "                    tip_amount +\n",
    "                    tolls_amount +\n",
    "                    sur ) = total_amount THEN 1\n",
    "                ELSE 0\n",
    "                END AS totals_equal\n",
    "                FROM conv\n",
    "            )\n",
    "            SELECT totals_equal, count(totals_equal) AS count\n",
    "            FROM calc\n",
    "            GROUP BY totals_equal\n",
    "            ORDER BY count DESC\n",
    "            \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "            CREATE OR REPLACE TEMP VIEW summed AS\n",
    "                WITH conv AS (\n",
    "                    SELECT fare_amount, extra, tip_amount, tolls_amount, total_amount,\n",
    "                        CASE WHEN mta_tax = true THEN 0.5 ELSE 0 END AS tax,\n",
    "                        CASE WHEN improvement_surcharge = true THEN 0.3 ELSE 0 END AS sur\n",
    "                    FROM ic_two_view\n",
    "                )\n",
    "                SELECT *, (fare_amount +\n",
    "                    extra +\n",
    "                    tax +\n",
    "                    tip_amount +\n",
    "                    tolls_amount +\n",
    "                    sur ) AS sum_costs\n",
    "                FROM conv\n",
    "            \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "            CREATE OR REPLACE TEMP VIEW diffs AS\n",
    "            SELECT *, total_amount - sum_costs AS totals_diff\n",
    "            FROM summed\n",
    "            \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- fare_amount: decimal(10,3) (nullable = true)\n",
      " |-- extra: decimal(10,3) (nullable = true)\n",
      " |-- tip_amount: decimal(10,3) (nullable = true)\n",
      " |-- tolls_amount: decimal(10,3) (nullable = true)\n",
      " |-- total_amount: decimal(10,3) (nullable = true)\n",
      " |-- tax: decimal(11,1) (nullable = false)\n",
      " |-- sur: decimal(11,1) (nullable = false)\n",
      " |-- sum_costs: decimal(17,3) (nullable = true)\n",
      " |-- totals_diff: decimal(18,3) (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check schema\n",
    "spark.sql(\"\"\"\n",
    "            SELECT *\n",
    "            FROM diffs\n",
    "            WHERE totals_diff > \"0\"\n",
    "            OR totals_diff < \"0\"\n",
    "            \"\"\").printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further EDA Driven Cleaning\n",
    "- Total sum error flag, remove values where not equal to; total_amount or, +1.95\n",
    "- drop where extra != 0, 0.5 or 1\n",
    "- drop distances > 60\n",
    "    - drop distances <= 0\n",
    "- drop times > 120mins\n",
    "    - drop times <= 0\n",
    "- drop fare amount > 260\n",
    "    - drop fare_amount <= 0\n",
    "- split out location 264 + 265\n",
    "- split out RatecodeID = 2 (airport)\n",
    "- drop RatecodeID = 6\n",
    "- drop tip > 40\n",
    "    - drop tip < 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datainitclean = spark.read.parquet(\"./data/out_init_clean.parquet\")\n",
    "datainitclean.createOrReplaceTempView(\"init_clean_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# need to convert floats to decimals for arithmetic\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "            CREATE OR REPLACE TEMP VIEW ic_two_view AS\n",
    "            SELECT VendorID,\n",
    "                    pickup_datetime,\n",
    "                    dropoff_datetime,\n",
    "                    passenger_count,\n",
    "                    CAST(trip_distance AS DECIMAL(10,3)),\n",
    "                    RatecodeID,\n",
    "                    PULocationID,\n",
    "                    DOLocationID,\n",
    "                    payment_type,\n",
    "                    CAST(fare_amount AS DECIMAL(10,3)),\n",
    "                    CAST(extra AS DECIMAL(10,3)),\n",
    "                    CAST(tip_amount AS DECIMAL(10,3)),\n",
    "                    CAST(tolls_amount AS DECIMAL(10,3)),\n",
    "                    CAST(total_amount AS DECIMAL(10,3)),\n",
    "                    store_and_fwd_flag,\n",
    "                    dispatched,\n",
    "                    yellow,\n",
    "                    tripdays,\n",
    "                    refunded_flag,\n",
    "                    mta_tax,\n",
    "                    improvement_surcharge\n",
    "            FROM init_clean_view\n",
    "        \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add fee\n",
    "spark.sql(\"\"\"\n",
    "            CREATE OR REPLACE TEMP VIEW conv AS\n",
    "                    SELECT *,\n",
    "                        CASE WHEN mta_tax = true THEN 0.5 ELSE 0 END AS mta_tax_fee,\n",
    "                        CASE WHEN improvement_surcharge = true THEN 0.3 ELSE 0 END AS surchage_fee\n",
    "                    FROM ic_two_view\n",
    "            \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add sum\n",
    "spark.sql(\"\"\"\n",
    "            CREATE OR REPLACE TEMP VIEW summed AS\n",
    "                SELECT *, (fare_amount +\n",
    "                    extra +\n",
    "                    mta_tax_fee +\n",
    "                    tip_amount +\n",
    "                    tolls_amount +\n",
    "                    surchage_fee ) AS sum_costs\n",
    "                FROM conv\n",
    "            \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add diffs\n",
    "spark.sql(\"\"\"\n",
    "            CREATE OR REPLACE TEMP VIEW diffs AS\n",
    "            SELECT *, total_amount - sum_costs AS totals_diff\n",
    "            FROM summed\n",
    "            \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#add unix timestamp\n",
    "spark.sql(\"\"\"\n",
    "            CREATE OR REPLACE TEMP VIEW trip_sec_view AS\n",
    "            SELECT *, (unix_timestamp(dropoff_datetime) - unix_timestamp(pickup_datetime)) AS trip_time_sec\n",
    "            FROM diffs\n",
    "            \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check impact of where difference is out of range\n",
    "spark.sql(\"\"\"\n",
    "            SELECT totals_diff\n",
    "            FROM trip_sec_view\n",
    "            WHERE NOT (totals_diff = 1.95 OR totals_diff = 0)\n",
    "            \"\"\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# relativley low impact, filter as planned\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "            CREATE OR REPLACE TEMP VIEW clean_2_1_view AS\n",
    "            SELECT *\n",
    "            FROM trip_sec_view\n",
    "            WHERE (totals_diff = 1.95 OR totals_diff = 0)\n",
    "            AND (extra = 0 OR extra = 0.5 OR extra = 1)\n",
    "            AND (trip_distance > 0 AND trip_distance <= 60)\n",
    "            AND (trip_time_sec > 0 AND trip_time_sec <= (120*60))\n",
    "            AND (fare_amount > 0 AND fare_amount <= 260)\n",
    "            AND PULocationID != 264\n",
    "            AND PULocationID != 265 \n",
    "            AND DOLocationID != 264 \n",
    "            AND DOLocationID != 265\n",
    "            AND RatecodeID != 2\n",
    "            AND RatecodeID != 6\n",
    "            AND (tip_amount >= 0 AND tip_amount <= 40)\n",
    "            \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create object from view\n",
    "cleancontinuous = spark.sql(\"\"\"\n",
    "            SELECT *\n",
    "            FROM clean_2_1_view\n",
    "            \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "- maxi taxi flag, passenger_count = 7-9\n",
    "- med taxi flag, passenger_count = 4-6\n",
    "- passengerless, passenger_count = 0\n",
    "- short trip flag (same pickup and dropoff)\n",
    "- total error flag (=1.95 out)\n",
    "- trip_time_sec\n",
    "- trip_time_min\n",
    "- trip_time_hour\n",
    "- avg_speed (mph)\n",
    "- AUC trip_meta = distance x time\n",
    "    - https://en.wikipedia.org/wiki/Absement\n",
    "    - https://physics.stackexchange.com/questions/158425/which-physical-entities-equal-distance-times-time\n",
    "- Time of dataset\n",
    "    - year\n",
    "    - month of dataset\n",
    "    - week of dataset,\n",
    "    - day of dataset\n",
    "- of year\n",
    "    - month\n",
    "    - week\n",
    "    - day\n",
    "- day of month\n",
    "- day of week\n",
    "- hour of day\n",
    "- min of hour\n",
    "- min of day\n",
    "\n",
    "- Add borough's\n",
    "\n",
    "- add service zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: byte (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- passenger_count: byte (nullable = true)\n",
      " |-- trip_distance: decimal(10,3) (nullable = true)\n",
      " |-- RatecodeID: byte (nullable = true)\n",
      " |-- PULocationID: short (nullable = true)\n",
      " |-- DOLocationID: short (nullable = true)\n",
      " |-- payment_type: byte (nullable = true)\n",
      " |-- fare_amount: decimal(10,3) (nullable = true)\n",
      " |-- extra: decimal(10,3) (nullable = true)\n",
      " |-- tip_amount: decimal(10,3) (nullable = true)\n",
      " |-- tolls_amount: decimal(10,3) (nullable = true)\n",
      " |-- total_amount: decimal(10,3) (nullable = true)\n",
      " |-- store_and_fwd_flag: boolean (nullable = true)\n",
      " |-- dispatched: boolean (nullable = true)\n",
      " |-- yellow: boolean (nullable = true)\n",
      " |-- tripdays: integer (nullable = true)\n",
      " |-- refunded_flag: boolean (nullable = true)\n",
      " |-- mta_tax: boolean (nullable = true)\n",
      " |-- improvement_surcharge: boolean (nullable = true)\n",
      " |-- mta_tax_fee: decimal(11,1) (nullable = false)\n",
      " |-- surchage_fee: decimal(11,1) (nullable = false)\n",
      " |-- sum_costs: decimal(17,3) (nullable = true)\n",
      " |-- totals_diff: decimal(18,3) (nullable = true)\n",
      " |-- trip_time_sec: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleancontinuous.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "360628"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleancontinuous.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleancontinuous.createOrReplaceTempView(\"clean_cont_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create new flags for taxi size, no passneger, short trip and near two error\n",
    "# calculate time related variables\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "            CREATE OR REPLACE TEMP VIEW eng_1_view AS\n",
    "            SELECT *,\n",
    "                BOOLEAN(CASE WHEN passenger_count BETWEEN 7 AND 9 THEN true\n",
    "                ELSE false END) AS flag_maxi_taxi,\n",
    "                BOOLEAN(CASE WHEN passenger_count BETWEEN 4 AND 6 THEN true\n",
    "                ELSE false END) AS flag_lg_taxi,\n",
    "                BOOLEAN(CASE WHEN passenger_count = 0 THEN true\n",
    "                ELSE false END) AS flag_no_passenger,\n",
    "                BOOLEAN(CASE WHEN PUlocationID = DOlocationID THEN true\n",
    "                ELSE false END) AS flag_short_trip,\n",
    "                BOOLEAN(CASE WHEN totals_diff = 1.95 THEN true\n",
    "                ELSE false END) AS flag_near_two_error,\n",
    "                CAST((trip_time_sec / 60) AS DECIMAL(20, 6)) AS trip_time_min,\n",
    "                ((trip_time_sec * trip_distance)/2) AS trip_meta,\n",
    "                DATEDIFF(pickup_datetime, \"2017-01-01\") AS PU_day_of_data,\n",
    "                YEAR(pickup_datetime) AS PU_year,\n",
    "                MONTH(pickup_datetime) AS PU_month_of_year,\n",
    "                WEEKOFYEAR(pickup_datetime) AS PU_week_of_year,\n",
    "                DAYOFMONTH(pickup_datetime) AS PU_day_of_month,\n",
    "                DAYOFWEEK(pickup_datetime) AS PU_day_of_week,\n",
    "                HOUR(pickup_datetime) AS PU_hour_of_day,\n",
    "                MINUTE(pickup_datetime) AS PU_min_of_hour\n",
    "            FROM clean_cont_view\n",
    "            \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|PU_hour_of_day|\n",
      "+--------------+\n",
      "|             0|\n",
      "|             1|\n",
      "|             2|\n",
      "|             3|\n",
      "|             4|\n",
      "|             5|\n",
      "|             6|\n",
      "|             7|\n",
      "|             8|\n",
      "|             9|\n",
      "|            10|\n",
      "|            11|\n",
      "|            12|\n",
      "|            13|\n",
      "|            14|\n",
      "|            15|\n",
      "|            16|\n",
      "|            17|\n",
      "|            18|\n",
      "|            19|\n",
      "|            20|\n",
      "|            21|\n",
      "|            22|\n",
      "|            23|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check structure of time output (0 or 24)\n",
    "spark.sql(\"\"\"\n",
    "            SELECT DISTINCT PU_hour_of_day\n",
    "            FROM eng_1_view\n",
    "            ORDER BY PU_hour_of_day\n",
    "            \"\"\").show(26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate derived time variables\n",
    "spark.sql(\"\"\"\n",
    "            CREATE OR REPLACE TEMP VIEW eng_2_view AS\n",
    "            SELECT *,\n",
    "              (trip_time_min / 60) AS trip_time_hour,\n",
    "              CASE WHEN PU_year = 2019 THEN 0\n",
    "              ELSE 1 END AS PU_year_of_data,\n",
    "              (PU_min_of_hour + (PU_hour_of_day * 60)) AS PU_min_of_day\n",
    "            FROM eng_1_view\n",
    "            \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# further derived time variables and speed\n",
    "spark.sql(\"\"\"\n",
    "            CREATE OR REPLACE TEMP VIEW eng_3_view AS\n",
    "            SELECT *,\n",
    "                (trip_distance / trip_time_hour) AS trip_avg_speed,\n",
    "                (PU_month_of_year + (PU_year_of_data * 12)) AS PU_month_of_data,\n",
    "                (PU_week_of_year + (PU_year_of_data * 52)) AS PU_week_of_data,\n",
    "                (PU_day_of_data - (PU_year_of_data * 365)) AS PU_day_of_year\n",
    "            FROM eng_2_view\n",
    "            \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstVarEng = spark.sql(\"\"\"\n",
    "                SELECT *\n",
    "                FROM eng_3_view\n",
    "                \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: byte (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- passenger_count: byte (nullable = true)\n",
      " |-- trip_distance: decimal(10,3) (nullable = true)\n",
      " |-- RatecodeID: byte (nullable = true)\n",
      " |-- PULocationID: short (nullable = true)\n",
      " |-- DOLocationID: short (nullable = true)\n",
      " |-- payment_type: byte (nullable = true)\n",
      " |-- fare_amount: decimal(10,3) (nullable = true)\n",
      " |-- extra: decimal(10,3) (nullable = true)\n",
      " |-- tip_amount: decimal(10,3) (nullable = true)\n",
      " |-- tolls_amount: decimal(10,3) (nullable = true)\n",
      " |-- total_amount: decimal(10,3) (nullable = true)\n",
      " |-- store_and_fwd_flag: boolean (nullable = true)\n",
      " |-- dispatched: boolean (nullable = true)\n",
      " |-- yellow: boolean (nullable = true)\n",
      " |-- tripdays: integer (nullable = true)\n",
      " |-- refunded_flag: boolean (nullable = true)\n",
      " |-- mta_tax: boolean (nullable = true)\n",
      " |-- improvement_surcharge: boolean (nullable = true)\n",
      " |-- mta_tax_fee: decimal(11,1) (nullable = false)\n",
      " |-- surchage_fee: decimal(11,1) (nullable = false)\n",
      " |-- sum_costs: decimal(17,3) (nullable = true)\n",
      " |-- totals_diff: decimal(18,3) (nullable = true)\n",
      " |-- trip_time_sec: long (nullable = true)\n",
      " |-- flag_maxi_taxi: boolean (nullable = false)\n",
      " |-- flag_lg_taxi: boolean (nullable = false)\n",
      " |-- flag_no_passenger: boolean (nullable = false)\n",
      " |-- flag_short_trip: boolean (nullable = false)\n",
      " |-- flag_near_two_error: boolean (nullable = false)\n",
      " |-- trip_time_min: decimal(20,6) (nullable = true)\n",
      " |-- trip_meta: decimal(34,6) (nullable = true)\n",
      " |-- PU_day_of_data: integer (nullable = true)\n",
      " |-- PU_year: integer (nullable = true)\n",
      " |-- PU_month_of_year: integer (nullable = true)\n",
      " |-- PU_week_of_year: integer (nullable = true)\n",
      " |-- PU_day_of_month: integer (nullable = true)\n",
      " |-- PU_day_of_week: integer (nullable = true)\n",
      " |-- PU_hour_of_day: integer (nullable = true)\n",
      " |-- PU_min_of_hour: integer (nullable = true)\n",
      " |-- trip_time_hour: decimal(23,9) (nullable = true)\n",
      " |-- PU_year_of_data: integer (nullable = false)\n",
      " |-- PU_min_of_day: integer (nullable = true)\n",
      " |-- trip_avg_speed: decimal(38,22) (nullable = true)\n",
      " |-- PU_month_of_data: integer (nullable = true)\n",
      " |-- PU_week_of_data: integer (nullable = true)\n",
      " |-- PU_day_of_year: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "firstVarEng.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add location groupings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstVarEng.createOrReplaceTempView(\"eng_3_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstVarEngCluster = spark.sql(\"\"\"\n",
    "                                SELECT *\n",
    "                                FROM eng_3_view\n",
    "                                CLUSTER BY PUlocationID, DOlocationID\n",
    "                                \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstVarEngCluster.createOrReplaceTempView(\"eng_3_view_cluster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[LocationID: string, Borough: string, Zone: string, service_zone: string]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxizone = spark.read.csv(\"Dataset/taxi+_zone_lookup.csv\", header = True)\n",
    "# minimze partitons of small dataset to reduce shuffle (similar to broadcast)\n",
    "taxizone.repartition(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxizone.createOrReplaceTempView(\"taxizone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[LocationID: string, Borough: string, Zone: string, service_zone: string]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sort to reduce shuffle\n",
    "spark.sql(\"\"\"\n",
    "        SELECT *\n",
    "        FROM taxizone\n",
    "        WHERE LocationID != 264\n",
    "        AND LocationID != 265\n",
    "        SORT BY LocationID\n",
    "        \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "        CREATE OR REPLACE TEMP VIEW PUtaxizone AS\n",
    "        SELECT LocationID AS PULocationID, Borough AS PUBorough, Zone AS PUZone, service_zone AS PUServiceZone\n",
    "        FROM taxizone\n",
    "        SORT BY PULocationID\n",
    "        \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "        CREATE OR REPLACE TEMP VIEW DOtaxizone AS\n",
    "        SELECT LocationID AS DOLocationID, Borough AS DOBorough, Zone AS DOZone, service_zone AS DOServiceZone\n",
    "        FROM taxizone\n",
    "        SORT BY DOLocationID\n",
    "        \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "        CREATE OR REPLACE TEMP VIEW PUjoin AS\n",
    "        SELECT eng_3_view_cluster.*, PUtaxizone.PUBorough, PUtaxizone.PUZone, PUtaxizone.PUServiceZone\n",
    "        FROM eng_3_view_cluster\n",
    "        LEFT JOIN PUtaxizone\n",
    "        ON eng_3_view_cluster.PULocationID = PUtaxizone.PULocationID\n",
    "        \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "        CREATE OR REPLACE TEMP VIEW DOjoin AS\n",
    "        SELECT PUjoin.*, DOtaxizone.DOBorough, DOtaxizone.DOZone, DOtaxizone.DOServiceZone\n",
    "        FROM PUjoin\n",
    "        LEFT JOIN DOtaxizone\n",
    "        ON PUjoin.DOLocationID = DOtaxizone.DOLocationID\n",
    "        \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "zonejoined = spark.sql(\"\"\"\n",
    "                        SELECT *\n",
    "                        FROM DOjoin\n",
    "                        \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: byte (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- passenger_count: byte (nullable = true)\n",
      " |-- trip_distance: decimal(10,3) (nullable = true)\n",
      " |-- RatecodeID: byte (nullable = true)\n",
      " |-- PULocationID: short (nullable = true)\n",
      " |-- DOLocationID: short (nullable = true)\n",
      " |-- payment_type: byte (nullable = true)\n",
      " |-- fare_amount: decimal(10,3) (nullable = true)\n",
      " |-- extra: decimal(10,3) (nullable = true)\n",
      " |-- tip_amount: decimal(10,3) (nullable = true)\n",
      " |-- tolls_amount: decimal(10,3) (nullable = true)\n",
      " |-- total_amount: decimal(10,3) (nullable = true)\n",
      " |-- store_and_fwd_flag: boolean (nullable = true)\n",
      " |-- dispatched: boolean (nullable = true)\n",
      " |-- yellow: boolean (nullable = true)\n",
      " |-- tripdays: integer (nullable = true)\n",
      " |-- refunded_flag: boolean (nullable = true)\n",
      " |-- mta_tax: boolean (nullable = true)\n",
      " |-- improvement_surcharge: boolean (nullable = true)\n",
      " |-- mta_tax_fee: decimal(11,1) (nullable = false)\n",
      " |-- surchage_fee: decimal(11,1) (nullable = false)\n",
      " |-- sum_costs: decimal(17,3) (nullable = true)\n",
      " |-- totals_diff: decimal(18,3) (nullable = true)\n",
      " |-- trip_time_sec: long (nullable = true)\n",
      " |-- flag_maxi_taxi: boolean (nullable = false)\n",
      " |-- flag_lg_taxi: boolean (nullable = false)\n",
      " |-- flag_no_passenger: boolean (nullable = false)\n",
      " |-- flag_short_trip: boolean (nullable = false)\n",
      " |-- flag_near_two_error: boolean (nullable = false)\n",
      " |-- trip_time_min: decimal(20,6) (nullable = true)\n",
      " |-- trip_meta: decimal(34,6) (nullable = true)\n",
      " |-- PU_day_of_data: integer (nullable = true)\n",
      " |-- PU_year: integer (nullable = true)\n",
      " |-- PU_month_of_year: integer (nullable = true)\n",
      " |-- PU_week_of_year: integer (nullable = true)\n",
      " |-- PU_day_of_month: integer (nullable = true)\n",
      " |-- PU_day_of_week: integer (nullable = true)\n",
      " |-- PU_hour_of_day: integer (nullable = true)\n",
      " |-- PU_min_of_hour: integer (nullable = true)\n",
      " |-- trip_time_hour: decimal(23,9) (nullable = true)\n",
      " |-- PU_year_of_data: integer (nullable = false)\n",
      " |-- PU_min_of_day: integer (nullable = true)\n",
      " |-- trip_avg_speed: decimal(38,22) (nullable = true)\n",
      " |-- PU_month_of_data: integer (nullable = true)\n",
      " |-- PU_week_of_data: integer (nullable = true)\n",
      " |-- PU_day_of_year: integer (nullable = true)\n",
      " |-- PUBorough: string (nullable = true)\n",
      " |-- PUZone: string (nullable = true)\n",
      " |-- PUServiceZone: string (nullable = true)\n",
      " |-- DOBorough: string (nullable = true)\n",
      " |-- DOZone: string (nullable = true)\n",
      " |-- DOServiceZone: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "zonejoined.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "zonejoined = zonejoined.drop('trip_avg_speed')\n",
    "zonejoined.createOrReplaceTempView(\"zonejoined_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recast speed\n",
    "spark.sql(\"\"\"\n",
    "        CREATE OR REPLACE TEMP VIEW checknew_view AS\n",
    "        SELECT *, CAST(ROUND((trip_distance / trip_time_hour), 2) AS DECIMAL(10,2)) AS trip_avg_speed\n",
    "        FROM zonejoined_view\n",
    "        \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+\n",
      "|summary|  trip_avg_speed|\n",
      "+-------+----------------+\n",
      "|  count|          360628|\n",
      "|   mean|       14.783688|\n",
      "| stddev|125.787304080096|\n",
      "|    min|            0.02|\n",
      "|    25%|            9.06|\n",
      "|    50%|           11.75|\n",
      "|    75%|           15.75|\n",
      "|    max|        32040.32|\n",
      "+-------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "        SELECT trip_avg_speed\n",
    "        FROM checknew_view\n",
    "        \"\"\").summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgspeed = spark.sql(\"\"\"\n",
    "                    SELECT trip_avg_speed, fare_amount, pickup_datetime, PU_week_of_data\n",
    "                    FROM checknew_view\n",
    "                    \"\"\").sample(fraction = 0.001).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import plotly.express as px\n",
    "#avgspeedFilter = avgspeed[avgspeed['trip_avg_speed'] < 60]\n",
    "#px.histogram(avgspeedFilter, x = \"trip_avg_speed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "#px.scatter(avgspeedFilter, x = \"trip_avg_speed\", y = \"fare_amount\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "#avgspeedFilterWeek = avgspeedFilter[avgspeedFilter['PU_week_of_data'] == 48]\n",
    "#px.scatter(avgspeedFilterWeek, x = \"pickup_datetime\", y = \"trip_avg_speed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check trip_meta\n",
    "Intended to capture relationship between time and distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkmeta = spark.sql(\"\"\"\n",
    "                        SELECT *\n",
    "                        FROM checknew_view\n",
    "                        \"\"\").drop(\"trip_meta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkmeta.createOrReplaceTempView(\"checkmeta_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "        CREATE OR REPLACE TEMP VIEW tripmeta AS\n",
    "        SELECT *,\n",
    "        ((trip_time_min * trip_distance)/2) AS trip_meta\n",
    "        FROM checkmeta_view\n",
    "        \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "tripmeta = spark.sql(\"\"\"\n",
    "                    SELECT trip_meta, fare_amount, pickup_datetime, PU_week_of_data, RatecodeID, PUBorough, yellow\n",
    "                    FROM tripmeta\n",
    "                    \"\"\").sample(fraction = 0.001).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trip_meta          347\n",
       "fare_amount        347\n",
       "pickup_datetime    347\n",
       "PU_week_of_data    347\n",
       "RatecodeID         347\n",
       "PUBorough          347\n",
       "yellow             347\n",
       "dtype: int64"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tripmeta.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the combined one month worth of data into a single file with an optimised file format.\n",
    "- structure\n",
    "\n",
    "    - yellow (colour) - RatecodeID\n",
    "          \\- Year of data\n",
    "              \\- Month of data\n",
    "                  \\- (bucket) day of data\n",
    "- clean using new metrics\n",
    "    - 0 < speed < 60\n",
    "    - fare > 2.5 https://www1.nyc.gov/site/tlc/passengers/taxi-fare.page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureout = spark.sql(\"\"\"\n",
    "                    SELECT *\n",
    "                    FROM tripmeta\n",
    "                    WHERE fare_amount >= 2.5\n",
    "                    AND trip_avg_speed > 0\n",
    "                    AND trip_avg_speed <= 60\n",
    "                    CLUSTER BY yellow, RatecodeID, PU_year_of_data, PU_month_of_data, pickup_datetime\n",
    "                    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureout.write.partitionBy(\"yellow\",\"RatecodeID\",\"PU_year_of_data\", \"PU_month_of_data\").parquet(\"./data/featureout.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureout = spark.read.parquet(\"./data/featureout.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: byte (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- passenger_count: byte (nullable = true)\n",
      " |-- trip_distance: decimal(10,3) (nullable = true)\n",
      " |-- PULocationID: short (nullable = true)\n",
      " |-- DOLocationID: short (nullable = true)\n",
      " |-- payment_type: byte (nullable = true)\n",
      " |-- fare_amount: decimal(10,3) (nullable = true)\n",
      " |-- extra: decimal(10,3) (nullable = true)\n",
      " |-- tip_amount: decimal(10,3) (nullable = true)\n",
      " |-- tolls_amount: decimal(10,3) (nullable = true)\n",
      " |-- total_amount: decimal(10,3) (nullable = true)\n",
      " |-- store_and_fwd_flag: boolean (nullable = true)\n",
      " |-- dispatched: boolean (nullable = true)\n",
      " |-- tripdays: integer (nullable = true)\n",
      " |-- refunded_flag: boolean (nullable = true)\n",
      " |-- mta_tax: boolean (nullable = true)\n",
      " |-- improvement_surcharge: boolean (nullable = true)\n",
      " |-- mta_tax_fee: decimal(11,1) (nullable = true)\n",
      " |-- surchage_fee: decimal(11,1) (nullable = true)\n",
      " |-- sum_costs: decimal(17,3) (nullable = true)\n",
      " |-- totals_diff: decimal(18,3) (nullable = true)\n",
      " |-- trip_time_sec: long (nullable = true)\n",
      " |-- flag_maxi_taxi: boolean (nullable = true)\n",
      " |-- flag_lg_taxi: boolean (nullable = true)\n",
      " |-- flag_no_passenger: boolean (nullable = true)\n",
      " |-- flag_short_trip: boolean (nullable = true)\n",
      " |-- flag_near_two_error: boolean (nullable = true)\n",
      " |-- trip_time_min: decimal(20,6) (nullable = true)\n",
      " |-- PU_day_of_data: integer (nullable = true)\n",
      " |-- PU_year: integer (nullable = true)\n",
      " |-- PU_month_of_year: integer (nullable = true)\n",
      " |-- PU_week_of_year: integer (nullable = true)\n",
      " |-- PU_day_of_month: integer (nullable = true)\n",
      " |-- PU_day_of_week: integer (nullable = true)\n",
      " |-- PU_hour_of_day: integer (nullable = true)\n",
      " |-- PU_min_of_hour: integer (nullable = true)\n",
      " |-- trip_time_hour: decimal(23,9) (nullable = true)\n",
      " |-- PU_min_of_day: integer (nullable = true)\n",
      " |-- PU_week_of_data: integer (nullable = true)\n",
      " |-- PU_day_of_year: integer (nullable = true)\n",
      " |-- PUBorough: string (nullable = true)\n",
      " |-- PUZone: string (nullable = true)\n",
      " |-- PUServiceZone: string (nullable = true)\n",
      " |-- DOBorough: string (nullable = true)\n",
      " |-- DOZone: string (nullable = true)\n",
      " |-- DOServiceZone: string (nullable = true)\n",
      " |-- trip_avg_speed: decimal(10,2) (nullable = true)\n",
      " |-- trip_meta: decimal(33,11) (nullable = true)\n",
      " |-- yellow: string (nullable = true)\n",
      " |-- RatecodeID: integer (nullable = true)\n",
      " |-- PU_year_of_data: integer (nullable = true)\n",
      " |-- PU_month_of_data: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "featureout.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "360343"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featureout.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureout.createOrReplaceTempView(\"start_view\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Day 1 of week = Sunday\n",
    "## Answer the list of provided business questions\n",
    "### a. For each year and month\n",
    "#### i. What was the total number of trips?\n",
    "##### Year (note:-Currently it is for only one month of the year 2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+\n",
      "|PU_year|total_trips|\n",
      "+-------+-----------+\n",
      "|   2019|     360343|\n",
      "+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "            CREATE OR REPLACE TEMP VIEW year_i AS\n",
    "            SELECT PU_year, count(*) AS total_trips\n",
    "            FROM start_view\n",
    "            GROUP BY PU_year\n",
    "            ORDER BY total_trips\n",
    "            \"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "            SELECT *\n",
    "            FROM year_i\n",
    "            \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------+\n",
      "|PU_month_of_data|total_trips|\n",
      "+----------------+-----------+\n",
      "|               1|     360343|\n",
      "+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "            CREATE OR REPLACE TEMP VIEW month_i AS\n",
    "            SELECT PU_month_of_data, count(*) AS total_trips\n",
    "            FROM start_view\n",
    "            GROUP BY PU_month_of_data\n",
    "            ORDER BY PU_month_of_data\n",
    "            \"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "            SELECT *\n",
    "            FROM month_i\n",
    "            \"\"\").show(26)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ii. Which weekday had the most trips?\n",
    "##### Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-----------------------+\n",
      "|PU_year|PU_day_most_trips|PU_day_most_trips_total|\n",
      "+-------+-----------------+-----------------------+\n",
      "|   2019|              Wed|                 183833|\n",
      "+-------+-----------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "            CREATE OR REPLACE TEMP VIEW year_ii AS\n",
    "            SELECT PU_year,\n",
    "            CASE WHEN PU_day_of_week = 1 THEN \"Sun\"\n",
    "            WHEN PU_day_of_week = 2 THEN \"Mon\"\n",
    "            WHEN PU_day_of_week = 3 THEN \"Tue\"\n",
    "            WHEN PU_day_of_week = 4 THEN \"Wed\"\n",
    "            WHEN PU_day_of_week = 5 THEN \"Thu\"\n",
    "            WHEN PU_day_of_week = 6 THEN \"Fri\"\n",
    "            WHEN PU_day_of_week = 7 THEN \"Sat\"\n",
    "            ELSE \"unknown\" END AS PU_day_most_trips,\n",
    "            COUNT(*) AS PU_day_most_trips_total\n",
    "            FROM start_view\n",
    "            GROUP BY PU_year, PU_day_of_week\n",
    "            HAVING PU_day_most_trips_total IN (\n",
    "                SELECT MAX(count)\n",
    "                    FROM (\n",
    "                        SELECT PU_year, PU_day_of_week, COUNT(*) as count\n",
    "                        FROM start_view\n",
    "                        GROUP BY PU_year, PU_day_of_week\n",
    "                    )\n",
    "                GROUP BY PU_year\n",
    "            )\n",
    "            ORDER BY PU_year\n",
    "            \"\"\")\n",
    "\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "            SELECT *\n",
    "            FROM year_ii\n",
    "            \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------------+-----------------------+\n",
      "|PU_month_of_data|PU_day_most_trips|PU_day_most_trips_total|\n",
      "+----------------+-----------------+-----------------------+\n",
      "|               1|              Wed|                 183833|\n",
      "+----------------+-----------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "            CREATE OR REPLACE TEMP VIEW month_ii AS\n",
    "            SELECT PU_month_of_data,\n",
    "            CASE WHEN PU_day_of_week = 1 THEN \"Sun\"\n",
    "            WHEN PU_day_of_week = 2 THEN \"Mon\"\n",
    "            WHEN PU_day_of_week = 3 THEN \"Tue\"\n",
    "            WHEN PU_day_of_week = 4 THEN \"Wed\"\n",
    "            WHEN PU_day_of_week = 5 THEN \"Thu\"\n",
    "            WHEN PU_day_of_week = 6 THEN \"Fri\"\n",
    "            WHEN PU_day_of_week = 7 THEN \"Sat\"\n",
    "            ELSE \"unknown\" END AS PU_day_most_trips,\n",
    "            COUNT(*) AS PU_day_most_trips_total\n",
    "            FROM start_view\n",
    "            GROUP BY PU_month_of_data, PU_day_of_week\n",
    "            HAVING PU_day_most_trips_total IN (\n",
    "                SELECT MAX(count)\n",
    "                FROM (\n",
    "                        SELECT PU_month_of_data, PU_day_of_week, COUNT(1) as count\n",
    "                        FROM start_view\n",
    "                        GROUP BY PU_month_of_data, PU_day_of_week\n",
    "                     )\n",
    "                GROUP BY PU_month_of_data\n",
    "            )\n",
    "            ORDER BY PU_month_of_data\n",
    "            \"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "            SELECT *\n",
    "            FROM month_ii\n",
    "            \"\"\").show(26)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### iii. Which hour of the day had the most trips?\n",
    "##### Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------------+\n",
      "|PU_year|PU_hour_most_trips|PU_hour_most_trips_total|\n",
      "+-------+------------------+------------------------+\n",
      "|   2019|                18|                   22239|\n",
      "+-------+------------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "            CREATE OR REPLACE TEMP VIEW year_iii AS\n",
    "            SELECT PU_year,\n",
    "            PU_hour_of_day AS PU_hour_most_trips,\n",
    "            COUNT(*) AS PU_hour_most_trips_total\n",
    "            FROM start_view\n",
    "            GROUP BY PU_year, PU_hour_of_day\n",
    "            HAVING PU_hour_most_trips_total IN (\n",
    "                SELECT MAX(count)\n",
    "                    FROM (\n",
    "                        SELECT PU_year, PU_hour_of_day, COUNT(*) as count\n",
    "                        FROM start_view\n",
    "                        GROUP BY PU_year, PU_hour_of_day\n",
    "                    )\n",
    "                GROUP BY PU_year\n",
    "            )\n",
    "            ORDER BY PU_year\n",
    "            \"\"\")\n",
    "\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "            SELECT *\n",
    "            FROM year_iii\n",
    "            \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------+------------------------+\n",
      "|PU_month_of_data|PU_hour_most_trips|PU_hour_most_trips_total|\n",
      "+----------------+------------------+------------------------+\n",
      "|               1|                18|                   22239|\n",
      "+----------------+------------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "            CREATE OR REPLACE TEMP VIEW month_iii AS\n",
    "            SELECT PU_month_of_data,\n",
    "            PU_hour_of_day AS PU_hour_most_trips,\n",
    "            COUNT(*) AS PU_hour_most_trips_total\n",
    "            FROM start_view\n",
    "            GROUP BY PU_month_of_data, PU_hour_of_day\n",
    "            HAVING PU_hour_most_trips_total IN (\n",
    "                SELECT MAX(count)\n",
    "                    FROM (\n",
    "                        SELECT PU_month_of_data, PU_hour_of_day, COUNT(*) as count\n",
    "                        FROM start_view\n",
    "                        GROUP BY PU_month_of_data, PU_hour_of_day\n",
    "                    )\n",
    "                GROUP BY PU_month_of_data\n",
    "            )\n",
    "            ORDER BY PU_month_of_data\n",
    "            \"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "            SELECT *\n",
    "            FROM month_iii\n",
    "            \"\"\").show(26)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### iv. What was the average number of passengers?\n",
    "##### Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------+\n",
      "|PU_year|average_passenger_count|\n",
      "+-------+-----------------------+\n",
      "|   2019|     1.6472860580058444|\n",
      "+-------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "            CREATE OR REPLACE TEMP VIEW year_iv AS\n",
    "            SELECT PU_year, AVG(passenger_count) AS average_passenger_count\n",
    "            FROM start_view\n",
    "            GROUP BY PU_year\n",
    "            ORDER BY PU_year\n",
    "            \"\"\")\n",
    "\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "            SELECT *\n",
    "            FROM year_iv\n",
    "            \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------------------+\n",
      "|PU_month_of_data|average_passenger_count|\n",
      "+----------------+-----------------------+\n",
      "|               1|     1.6472860580058444|\n",
      "+----------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "            CREATE OR REPLACE TEMP VIEW month_iv AS\n",
    "            SELECT PU_month_of_data, AVG(passenger_count) AS average_passenger_count\n",
    "            FROM start_view\n",
    "            GROUP BY PU_month_of_data\n",
    "            ORDER BY PU_month_of_data\n",
    "            \"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "            SELECT *\n",
    "            FROM month_iv\n",
    "            \"\"\").show(26)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### v. What was the average amount paid per trip (total_amount)?\n",
    "##### Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|PU_year|average_total_amount|\n",
      "+-------+--------------------+\n",
      "|   2019|          14.6329136|\n",
      "+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "            CREATE OR REPLACE TEMP VIEW year_v AS\n",
    "            SELECT PU_year, AVG(total_amount) AS average_total_amount\n",
    "            FROM start_view\n",
    "            GROUP BY PU_year\n",
    "            ORDER BY PU_year\n",
    "            \"\"\")\n",
    "\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "            SELECT *\n",
    "            FROM year_v\n",
    "            \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+\n",
      "|PU_month_of_data|average_total_amount|\n",
      "+----------------+--------------------+\n",
      "|               1|          14.6329136|\n",
      "+----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "            CREATE OR REPLACE TEMP VIEW month_v AS\n",
    "            SELECT PU_month_of_data, AVG(total_amount) AS average_total_amount\n",
    "            FROM start_view\n",
    "            GROUP BY PU_month_of_data\n",
    "            ORDER BY PU_month_of_data\n",
    "            \"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "            SELECT *\n",
    "            FROM month_v\n",
    "            \"\"\").show(26)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### vi. What was the average amount paid per passenger (total_amount)?\n",
    "##### Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------+\n",
      "|PU_year|avg_total_per_passenger|\n",
      "+-------+-----------------------+\n",
      "|   2019|                 11.748|\n",
      "+-------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "             CREATE OR REPLACE TEMP VIEW year_vi AS\n",
    "             SELECT PU_year,\n",
    "             CAST(AVG(total_amount / passenger_count) AS DECIMAL(6,3)) AS avg_total_per_passenger\n",
    "             FROM (\n",
    "                SELECT PU_year,\n",
    "                total_amount,\n",
    "                CASE WHEN passenger_count = 0 THEN 1\n",
    "                ELSE passenger_count\n",
    "                END AS passenger_count\n",
    "                FROM start_view\n",
    "             )\n",
    "             GROUP BY PU_year\n",
    "             ORDER BY PU_year\n",
    "            \"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "            SELECT *\n",
    "            FROM year_vi\n",
    "            \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------------------+\n",
      "|PU_month_of_data|avg_total_per_passenger|\n",
      "+----------------+-----------------------+\n",
      "|               1|                 11.748|\n",
      "+----------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "            CREATE OR REPLACE TEMP VIEW month_vi AS\n",
    "             SELECT PU_month_of_data,\n",
    "             CAST(AVG(total_amount / passenger_count) AS DECIMAL(6,3)) AS avg_total_per_passenger\n",
    "             FROM (\n",
    "                SELECT PU_month_of_data,\n",
    "                total_amount,\n",
    "                CASE WHEN passenger_count = 0 THEN 1\n",
    "                ELSE passenger_count\n",
    "                END AS passenger_count\n",
    "                FROM start_view\n",
    "             )\n",
    "             GROUP BY PU_month_of_data\n",
    "             ORDER BY PU_month_of_data\n",
    "            \"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "            SELECT *\n",
    "            FROM month_vi\n",
    "            \"\"\").show(26)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. For each taxi colour\n",
    "##### i. What was the average, median, minimum and maximum trip duration in seconds?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------+---------------+------------+------------+\n",
      "|yellow| average_trip_sec|median_trip_sec|min_trip_sec|max_trip_sec|\n",
      "+------+-----------------+---------------+------------+------------+\n",
      "| false|705.6616501499959|            574|           2|        7136|\n",
      "+------+-----------------+---------------+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "            SELECT yellow,\n",
    "            AVG(trip_time_sec) AS average_trip_sec,\n",
    "            percentile_approx(trip_time_sec, 0.5) AS median_trip_sec,\n",
    "            MIN(trip_time_sec) AS min_trip_sec,\n",
    "            MAX(trip_time_sec) AS max_trip_sec\n",
    "            FROM start_view\n",
    "            GROUP BY yellow\n",
    "            \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ii. What was the average, median, minimum and maximum trip distance in km?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------------+-----------------------+--------------------+--------------------+\n",
      "|yellow|average_trip_distanc_km|median_trip_distance_km|min_trip_distance_km|max_trip_distance_km|\n",
      "+------+-----------------------+-----------------------+--------------------+--------------------+\n",
      "| false|        4.5988111399017|              2.7358848|         0.016093440|        80.064864000|\n",
      "+------+-----------------------+-----------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "            SELECT yellow,\n",
    "            AVG(trip_distance * 1.609344) AS average_trip_distanc_km,\n",
    "            percentile_approx(trip_distance * 1.609344, 0.5) AS median_trip_distance_km,\n",
    "            MIN(trip_distance * 1.609344) AS min_trip_distance_km,\n",
    "            MAX(trip_distance * 1.609344) AS max_trip_distance_km\n",
    "            FROM start_view\n",
    "            GROUP BY yellow\n",
    "            \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### iii. What was the average, median, minimum and maximum speed in km per hour?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+------------------+--------------------+--------------------+\n",
      "|yellow|   average_trip_kmph|  median_trip_kmph|       min_trip_kmph|       max_trip_kmph|\n",
      "+------+--------------------+------------------+--------------------+--------------------+\n",
      "| false|21.40461549665005...|18.892300095584897|0.029491669286134...|96.56067862427144...|\n",
      "+------+--------------------+------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "            SELECT yellow,\n",
    "            AVG((trip_distance * 1.609344) / trip_time_hour) AS average_trip_kmph,\n",
    "            percentile_approx((trip_distance * 1.609344) / trip_time_hour, 0.5) AS median_trip_kmph,\n",
    "            MIN((trip_distance * 1.609344) / trip_time_hour) AS min_trip_kmph,\n",
    "            MAX((trip_distance * 1.609344) / trip_time_hour) AS max_trip_kmph\n",
    "            FROM start_view\n",
    "            GROUP BY yellow\n",
    "            \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. What was the percentage of trips where the driver received tips?\n",
    "    - ignores cash tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|tipped_trips_of_all_trips|\n",
      "+-------------------------+\n",
      "|                   59.38%|\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "            SELECT FORMAT_STRING(\"%s%%\", \n",
    "            FORMAT_NUMBER(COUNT(CASE WHEN tip_amount > 0 THEN 1 ELSE null END) / COUNT(*) * 100, 2)\n",
    "            ) AS tipped_trips_of_all_trips\n",
    "            FROM start_view\n",
    "            \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d. For trips where the driver received tips, What was the percentage where the driver received tips of at least $10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+\n",
      "|tip_greater_than_10_of_tipped_trips|\n",
      "+-----------------------------------+\n",
      "|                              1.21%|\n",
      "+-----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "            SELECT FORMAT_STRING(\"%s%%\", \n",
    "            FORMAT_NUMBER(COUNT(\n",
    "            CASE WHEN tip_amount > 10 THEN 1 ELSE null END) / COUNT(\n",
    "            CASE WHEN tip_amount > 0 THEN 1 ELSE null END) * 100, 2)\n",
    "            ) AS tip_greater_than_10_of_tipped_trips\n",
    "            FROM start_view\n",
    "            \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e. Classify each trip into bins of durations:\n",
    "- Under 5 Mins\n",
    "- From 5 mins to 10 mins\n",
    "- From 10 mins to 20 mins\n",
    "- From 20 mins to 30 mins\n",
    "- At least 30 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "            CREATE OR REPLACE TEMP VIEW binned_trips_view AS\n",
    "            SELECT *, CASE WHEN trip_time_min < 5 THEN \"a-under-five\"\n",
    "                WHEN (trip_time_min >= 5 AND trip_time_min < 10) THEN \"b-five-ten\"\n",
    "                WHEN (trip_time_min >= 10 AND trip_time_min < 20) THEN \"c-ten-twenty\"\n",
    "                WHEN (trip_time_min >= 20 AND trip_time_min < 30) THEN \"d-twenty-thirty\"\n",
    "                WHEN trip_time_min >= 30 THEN \"e-over-thirty\"\n",
    "                ELSE \"unknown\" END AS trip_time_bins\n",
    "            FROM start_view\n",
    "            \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e (cont). Then for each bins, calculate\n",
    "- Average speed (km per hour)\n",
    "- Average distance per dollar (km per $)\n",
    "- extra:\n",
    "    - Average mins per dollar\n",
    "    - Volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------------+----------------------+----------------------------+-----------+\n",
      "| trip_time_bins|average_trip_kmph|average_trip_kmpdollar|average_trip_mins_per_dollar|trip_volume|\n",
      "+---------------+-----------------+----------------------+----------------------------+-----------+\n",
      "|   a-under-five|           20.926|                 0.176|                       0.528|      69139|\n",
      "|     b-five-ten|           18.603|                 0.236|                       0.790|     119764|\n",
      "|   c-ten-twenty|           21.340|                 0.300|                       0.931|     120530|\n",
      "|d-twenty-thirty|           28.058|                 0.366|                       0.911|      36813|\n",
      "|  e-over-thirty|           30.726|                 0.404|                       0.909|      14097|\n",
      "+---------------+-----------------+----------------------+----------------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "            SELECT trip_time_bins, ROUND(AVG((trip_distance * 1.609344) / trip_time_hour), 3) AS average_trip_kmph,\n",
    "            ROUND(AVG((trip_distance * 1.609344) / total_amount), 3) AS average_trip_kmpdollar,\n",
    "            ROUND(AVG(trip_time_min / total_amount), 3) AS average_trip_mins_per_dollar,\n",
    "            COUNT(*) AS trip_volume\n",
    "            FROM binned_trips_view\n",
    "            GROUP BY trip_time_bins\n",
    "            ORDER BY trip_time_bins\n",
    "            \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: byte (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- passenger_count: byte (nullable = true)\n",
      " |-- trip_distance: decimal(10,3) (nullable = true)\n",
      " |-- PULocationID: short (nullable = true)\n",
      " |-- DOLocationID: short (nullable = true)\n",
      " |-- payment_type: byte (nullable = true)\n",
      " |-- fare_amount: decimal(10,3) (nullable = true)\n",
      " |-- extra: decimal(10,3) (nullable = true)\n",
      " |-- tip_amount: decimal(10,3) (nullable = true)\n",
      " |-- tolls_amount: decimal(10,3) (nullable = true)\n",
      " |-- total_amount: decimal(10,3) (nullable = true)\n",
      " |-- store_and_fwd_flag: boolean (nullable = true)\n",
      " |-- dispatched: boolean (nullable = true)\n",
      " |-- tripdays: integer (nullable = true)\n",
      " |-- refunded_flag: boolean (nullable = true)\n",
      " |-- mta_tax: boolean (nullable = true)\n",
      " |-- improvement_surcharge: boolean (nullable = true)\n",
      " |-- mta_tax_fee: decimal(11,1) (nullable = true)\n",
      " |-- surchage_fee: decimal(11,1) (nullable = true)\n",
      " |-- sum_costs: decimal(17,3) (nullable = true)\n",
      " |-- totals_diff: decimal(18,3) (nullable = true)\n",
      " |-- trip_time_sec: long (nullable = true)\n",
      " |-- flag_maxi_taxi: boolean (nullable = true)\n",
      " |-- flag_lg_taxi: boolean (nullable = true)\n",
      " |-- flag_no_passenger: boolean (nullable = true)\n",
      " |-- flag_short_trip: boolean (nullable = true)\n",
      " |-- flag_near_two_error: boolean (nullable = true)\n",
      " |-- trip_time_min: decimal(20,6) (nullable = true)\n",
      " |-- PU_day_of_data: integer (nullable = true)\n",
      " |-- PU_year: integer (nullable = true)\n",
      " |-- PU_month_of_year: integer (nullable = true)\n",
      " |-- PU_week_of_year: integer (nullable = true)\n",
      " |-- PU_day_of_month: integer (nullable = true)\n",
      " |-- PU_day_of_week: integer (nullable = true)\n",
      " |-- PU_hour_of_day: integer (nullable = true)\n",
      " |-- PU_min_of_hour: integer (nullable = true)\n",
      " |-- trip_time_hour: decimal(23,9) (nullable = true)\n",
      " |-- PU_min_of_day: integer (nullable = true)\n",
      " |-- PU_week_of_data: integer (nullable = true)\n",
      " |-- PU_day_of_year: integer (nullable = true)\n",
      " |-- PUBorough: string (nullable = true)\n",
      " |-- PUZone: string (nullable = true)\n",
      " |-- PUServiceZone: string (nullable = true)\n",
      " |-- DOBorough: string (nullable = true)\n",
      " |-- DOZone: string (nullable = true)\n",
      " |-- DOServiceZone: string (nullable = true)\n",
      " |-- trip_avg_speed: decimal(10,2) (nullable = true)\n",
      " |-- trip_meta: decimal(33,11) (nullable = true)\n",
      " |-- yellow: string (nullable = true)\n",
      " |-- RatecodeID: integer (nullable = true)\n",
      " |-- PU_year_of_data: integer (nullable = true)\n",
      " |-- PU_month_of_data: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "featureout.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureout.createOrReplaceTempView(\"start_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = featureout.sample(fraction = 0.001, seed = 55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.createOrReplaceTempView(\"sample_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop some uneeded cols\n",
    "proc = sample.drop(\"PU_year\", \"tripdays\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "procRateOne = proc.filter(proc.RatecodeID == 1)\n",
    "procRateOne = procRateOne.withColumn(\"yellow\", sparkle.col(\"yellow\").astype(BooleanType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "procRateOne.createOrReplaceTempView(\"procRateOne_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "procRateOne = spark.sql(\"\"\"\n",
    "                        SELECT *, UNIX_TIMESTAMP(pickup_datetime) AS pickup_datetime_unix,\n",
    "                        UNIX_TIMESTAMP(dropoff_datetime) AS dropoff_datetime_unix\n",
    "                        FROM procRateOne_view\n",
    "                        \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = [\"PULocationID\",\n",
    "        \"DOLocationID\",\n",
    "        \"PUBorough\",\n",
    "        \"PUZone\",\n",
    "        \"PUServiceZone\",\n",
    "        \"DOBorough\",\n",
    "        \"DOZone\",\n",
    "        \"DOServiceZone\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "stages = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create stages to encode all categoricals through string indexer then to one hot encoder\n",
    "for cat in cats:\n",
    "    col_indexer = StringIndexer(inputCol=cat, outputCol=f\"{cat}_ind\")\n",
    "    col_encoder = OneHotEncoderEstimator(inputCols=[f\"{cat}_ind\"], outputCols=[f\"{cat}_ohe\"])\n",
    "    stages += [col_indexer, col_encoder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats_ohe = [f\"{cat}_ohe\" for cat in cats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = [\"VendorID\",\n",
    "        \"pickup_datetime_unix\",\n",
    "        \"dropoff_datetime_unix\",\n",
    "        \"passenger_count\",\n",
    "        \"trip_distance\",\n",
    "        \"payment_type\",\n",
    "        \"extra\",\n",
    "        \"tip_amount\",\n",
    "        \"tolls_amount\",\n",
    "        \"store_and_fwd_flag\",\n",
    "        \"dispatched\",\n",
    "        \"refunded_flag\",\n",
    "        \"mta_tax\",\n",
    "        \"improvement_surcharge\",\n",
    "        \"trip_time_sec\",\n",
    "        \"flag_maxi_taxi\",\n",
    "        \"flag_lg_taxi\",\n",
    "        \"flag_no_passenger\",\n",
    "        \"flag_short_trip\",\n",
    "        \"flag_near_two_error\",\n",
    "        \"trip_time_min\",\n",
    "        \"PU_day_of_data\",\n",
    "        \"trip_time_hour\",\n",
    "        \"PU_week_of_data\",\n",
    "        \"trip_avg_speed\",\n",
    "        \"trip_meta\",\n",
    "        \"yellow\",\n",
    "        \"PU_year_of_data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assemble using vector assembler into feature column\n",
    "assembler = VectorAssembler(inputCols=cats_ohe + nums, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stages += [label, assembler]\n",
    "stages += [assembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pipeline\n",
    "pipeline = Pipeline(stages=stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit pipeline to  data\n",
    "pipeline_model = pipeline.fit(procRateOne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform dataset with pipeline (applying transformatons by column name)\n",
    "pipeOut = pipeline_model.transform(procRateOne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create view so spark sql unix timestamp can be used to prevent missmatch timezones\n",
    "pipeOut.createOrReplaceTempView(\"pipeout_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change outcome to label as required by pyspark ml\n",
    "train_data = spark.sql(\"\"\"\n",
    "                        SELECT features, total_amount AS label\n",
    "                        FROM pipeout_view\n",
    "                        WHERE pickup_datetime_unix < unix_timestamp(\"2019-01-31 00:00:00\")\n",
    "                        \"\"\")\n",
    "\n",
    "test_data = spark.sql(\"\"\"\n",
    "                        SELECT features, total_amount AS label\n",
    "                        FROM pipeout_view\n",
    "                        WHERE pickup_datetime_unix >= unix_timestamp(\"2019-01-31 00:00:00\")\n",
    "                        \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|            features| label|\n",
      "+--------------------+------+\n",
      "|(317,[10,100,140,...| 6.960|\n",
      "|(317,[9,64,140,14...| 8.300|\n",
      "|(317,[50,132,192,...|18.800|\n",
      "|(317,[5,64,140,14...| 6.300|\n",
      "|(317,[11,66,140,1...| 5.760|\n",
      "|(317,[1,80,140,14...|42.580|\n",
      "|(317,[9,84,140,14...|10.560|\n",
      "|(317,[24,83,140,1...| 9.350|\n",
      "|(317,[8,67,140,14...| 7.880|\n",
      "|(317,[5,65,140,14...| 7.300|\n",
      "|(317,[47,63,140,1...| 9.300|\n",
      "|(317,[30,106,140,...|10.700|\n",
      "|(317,[15,120,141,...|37.850|\n",
      "|(317,[11,67,140,1...| 8.760|\n",
      "|(317,[15,86,141,1...|51.060|\n",
      "|(317,[41,81,140,1...| 8.400|\n",
      "|(317,[40,67,140,1...|11.750|\n",
      "|(317,[36,100,140,...| 8.750|\n",
      "|(317,[12,79,140,1...|17.760|\n",
      "|(317,[26,85,140,1...|15.950|\n",
      "+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show minimised dataset with features vector column and label output column\n",
    "train_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "glm = GeneralizedLinearRegression(family=\"gaussian\", link=\"identity\", maxIter=5, regParam=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = glm.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create predictions\n",
    "train_preds = model.transform(train_data)\n",
    "test_preds = model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = RegressionEvaluator(metricName='rmse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+------------------+\n",
      "|            features| label|        prediction|\n",
      "+--------------------+------+------------------+\n",
      "|(317,[10,100,140,...| 6.960|7.1537970440240315|\n",
      "|(317,[9,64,140,14...| 8.300|  8.34966574691316|\n",
      "|(317,[50,132,192,...|18.800|18.808501453742792|\n",
      "|(317,[5,64,140,14...| 6.300| 6.545561579327114|\n",
      "|(317,[11,66,140,1...| 5.760| 5.342554414551159|\n",
      "|(317,[1,80,140,14...|42.580| 43.27442648695251|\n",
      "|(317,[9,84,140,14...|10.560|10.835104541624787|\n",
      "|(317,[24,83,140,1...| 9.350| 9.341703361595137|\n",
      "|(317,[8,67,140,14...| 7.880|  7.30642526538395|\n",
      "|(317,[5,65,140,14...| 7.300| 7.772162316757203|\n",
      "|(317,[47,63,140,1...| 9.300| 9.315792287501608|\n",
      "|(317,[30,106,140,...|10.700|10.548232054940854|\n",
      "|(317,[15,120,141,...|37.850|37.851803495009335|\n",
      "|(317,[11,67,140,1...| 8.760| 6.967902820775635|\n",
      "|(317,[15,86,141,1...|51.060| 49.47836953049682|\n",
      "|(317,[41,81,140,1...| 8.400| 8.319851729401307|\n",
      "|(317,[40,67,140,1...|11.750|10.988011618248493|\n",
      "|(317,[36,100,140,...| 8.750| 9.195738224611887|\n",
      "|(317,[12,79,140,1...|17.760|17.407512624619812|\n",
      "|(317,[26,85,140,1...|15.950|15.741500115131203|\n",
      "+--------------------+------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# inpsect\n",
    "train_preds.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vis = test_preds.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
